# Adaptive Information Retrieval System

**Deep Reinforcement Learning for Query Reformulation in Multi-Stage Retrieval**

Há»‡ thá»‘ng tÃ¬m kiáº¿m thÃ´ng tin sá»­ dá»¥ng RL Ä‘á»ƒ cáº£i thiá»‡n Recall@100 thÃ´ng qua query reformulation vÃ  RRF fusion.

---

## ğŸ¯ Tá»•ng Quan

Há»‡ thá»‘ng giáº£i quyáº¿t váº¥n Ä‘á» **bounded recall** trong multi-stage IR: documents khÃ´ng Ä‘Æ°á»£c retrieve á»Ÿ stage 1 sáº½ khÃ´ng bao giá» Ä‘Æ°á»£c re-ranker xem xÃ©t. Pipeline 4 giai Ä‘oáº¡n:

```
Query â†’ [Stage 0] Candidate Mining â†’ [Stage 1] RL Reformulation 
      â†’ [Stage 2] Multi-Query + RRF Fusion â†’ [Stage 3] BERT Re-rank â†’ Results
```

### Kiáº¿n trÃºc Pipeline

**Stage 0: Candidate Term Mining**
- Input: Original query
- Method: TF-IDF, BM25 contribution tá»« top-k documents
- Output: ~50 candidate expansion terms vá»›i features

**Stage 1: RL Query Reformulation**
- Agent: Actor-Critic Transformer (2.2M params)
- State: (query_emb, current_query_emb, candidate_embs, candidate_features)
- Action: Select term tá»« candidates (hoáº·c STOP)
- Reward: Term quality + relevance signal + length penalty
- Output: m query variants (m=4 máº·c Ä‘á»‹nh)

**Stage 2: Multi-Query Retrieval + RRF Fusion**
- Retrieve vá»›i má»—i query variant
- Fuse rankings vá»›i Reciprocal Rank Fusion (k=60)
- Output: Unified ranked list (Recall tÄƒng ~30%)

**Stage 3: BERT Cross-Encoder Re-ranking**
- Model: MS MARCO MiniLM-L-6-v2
- Re-rank top-50 candidates
- Output: Final ranked results

---

## ğŸ“Š Káº¿t Quáº£ (MS Academic Dataset)

| Method | Recall@10 | Recall@100 | MRR@10 | Latency |
|--------|-----------|------------|--------|---------|
| BM25 Baseline | 0.168 | 0.204 | 0.220 | 50ms |
| BM25 + RM3 | 0.189 | 0.235 | 0.240 | 120ms |
| **RL + RRF (Ours)** | **0.215** | **0.268** | **0.292** | 280ms |
| + BERT Re-rank | 0.227 | 0.268 | 0.308 | 1200ms |

**Cáº£i thiá»‡n:** +31% Recall@100, +33% MRR@10 so vá»›i BM25 baseline

---

## ğŸš€ Quick Start

### 1. CÃ i Ä‘áº·t

```bash
# Clone repository
cd adaptive-information-retrieval/adaptive-ir-system

# Install dependencies
pip install -r requirements.txt

# Setup Java for Pyserini (náº¿u chÆ°a cÃ³)
export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64

# Download NLTK data
python -c "import nltk; nltk.download('stopwords')"
```

### 2. Chuáº©n bá»‹ dá»¯ liá»‡u

Data structure:
```
Query Reformulator/
â”œâ”€â”€ msa_dataset.hdf5          # Queries + qrels
â”œâ”€â”€ msa_corpus.hdf5           # Document corpus (480K docs)
â””â”€â”€ D_cbow_pdw_8B.pkl         # Word2Vec embeddings (500-dim)
```

Files nÃ y cáº§n Ä‘áº·t á»Ÿ `../Query Reformulator/` (parent cá»§a `adaptive-ir-system/`)

### 3. Training

**Quick Training (2 epochs, ~9 hours trÃªn 2x Tesla T4):**
```bash
python train_quickly.py --config ./configs/msa_quick_config.yaml
```

**Full Training (10 epochs, khuyáº¿n nghá»‹ cho production):**
```bash
# Edit configs/msa_quick_config.yaml: num_epochs: 10
python train_quickly.py --config ./configs/msa_quick_config.yaml --epochs 10
```

**Key Hyperparameters:**
- `collect_batch_size: 128` - Sá»‘ episodes xá»­ lÃ½ song song
- `num_query_variants: 4` - Sá»‘ query variants Ä‘á»ƒ fuse
- `use_amp: true` - Mixed precision FP16 (báº¯t buá»™c vá»›i GPU nhá»)
- `reward_mode: improved` - Improved reward vá»›i term quality signal

Checkpoint Ä‘Æ°á»£c lÆ°u táº¡i: `checkpoints_msa_optimized/best_model.pt`

### 4. Evaluation

**Fast Evaluation (BM25 only, khÃ´ng BERT):**
```bash
python eval_checkpoint_optimized.py \
    --checkpoint checkpoints_msa_optimized/best_model.pt \
    --split valid \
    --num-queries 500
```

**Full Evaluation (so sÃ¡nh táº¥t cáº£ methods):**
```bash
python evaluate_full.py \
    --checkpoint checkpoints_msa_optimized/best_model.pt \
    --split valid \
    --num-queries 1000 \
    --no-bert  # Bá» flag nÃ y náº¿u muá»‘n test BERT
```

Output:
```
Method                         R@10       R@100      MRR@10     MAP        Latency
BM25 Baseline                  0.1680     0.2040     0.2200     0.1850     50ms
BM25 + RM3                     0.1890     0.2350     0.2400     0.2010     120ms
RL + RRF (m=4)                 0.2150     0.2680     0.2920     0.2280     280ms
```

### 5. Inference (Production)

```bash
python inference.py \
    --checkpoint checkpoints_msa_optimized/best_model.pt \
    --query "machine learning deep neural networks"
```

Output:
```json
{
  "original_query": "machine learning deep neural networks",
  "reformulated_queries": [
    "machine learning deep neural networks",
    "machine learning deep neural networks convolutional",
    "machine learning deep neural networks training optimization",
    ...
  ],
  "results": [
    {"doc_id": "12345", "score": 0.95, "title": "..."},
    ...
  ],
  "latency": {
    "candidate_mining": 0.08,
    "rl_reformulation": 0.12,
    "retrieval_fusion": 0.15,
    "total": 0.35
  }
}
```

---

## ğŸ“ Cáº¥u TrÃºc Source Code

```
adaptive-ir-system/
â”œâ”€â”€ train_quickly.py                    # Main training script
â”œâ”€â”€ eval_checkpoint_optimized.py        # Fast evaluation
â”œâ”€â”€ evaluate_full.py                    # Comprehensive evaluation
â”œâ”€â”€ inference.py                        # Production inference
â”œâ”€â”€ requirements.txt                    # Dependencies
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ msa_config.yaml                # Full training config
â”‚   â””â”€â”€ msa_quick_config.yaml          # Quick training config (2 epochs)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ adaptive_pipeline.py       # End-to-end 4-stage pipeline
â”‚   â”œâ”€â”€ rl_agent/
â”‚   â”‚   â””â”€â”€ agent.py                   # Actor-Critic policy network
â”‚   â”œâ”€â”€ candidate_mining/
â”‚   â”‚   â””â”€â”€ term_miner.py              # TF-IDF + BM25 candidate extraction
â”‚   â”œâ”€â”€ fusion/
â”‚   â”‚   â””â”€â”€ rrf.py                     # Reciprocal Rank Fusion
â”‚   â”œâ”€â”€ reranker/
â”‚   â”‚   â””â”€â”€ bert_reranker.py           # BERT cross-encoder wrapper
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ metrics.py                 # IR metrics (Recall, MRR, nDCG, MAP)
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ train_rl_quickly.py        # Optimized PPO training loop
â”‚   â”œâ”€â”€ baselines/
â”‚   â”‚   â””â”€â”€ rm3.py                     # RM3 pseudo-relevance feedback
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ legacy_loader.py           # HDF5 dataset loader
â”‚       â”œâ”€â”€ legacy_embeddings.py       # Word2Vec loader
â”‚       â”œâ”€â”€ simple_searcher.py         # In-memory BM25 search
â”‚       â”œâ”€â”€ helpers.py                 # Utilities (logging, config, etc.)
â”‚       â””â”€â”€ huggingface_uploader.py    # HF Hub integration
â””â”€â”€ checkpoints_msa_optimized/
    â”œâ”€â”€ best_model.pt                  # Best checkpoint (theo MRR)
    â”œâ”€â”€ checkpoint_epoch_N.pt          # Per-epoch checkpoints
    â””â”€â”€ final_model.pt                 # Checkpoint cuá»‘i cÃ¹ng
```

---

## âš™ï¸ Configuration

File config chÃ­nh: `configs/msa_quick_config.yaml`

**Sections quan trá»ng:**

```yaml
# Data paths
data:
  data_dir: ../Query Reformulator
  dataset_path: msa_dataset.hdf5
  corpus_path: msa_corpus.hdf5

# Embeddings
embeddings:
  type: legacy  # Word2Vec 500-dim
  path: ../Query Reformulator/D_cbow_pdw_8B.pkl

# RL Agent
rl_agent:
  embedding_dim: 500
  hidden_dim: 256
  num_query_variants: 4         # m variants Ä‘á»ƒ fuse
  max_steps_per_episode: 5      # Max terms to add per query
  learning_rate: 0.0003
  gamma: 0.99                   # Discount factor
  clip_epsilon: 0.2             # PPO clipping

# Training
training:
  num_epochs: 2                 # Quick: 2, Full: 10
  collect_batch_size: 128       # Episodes per batch (giáº£m náº¿u OOM)
  use_amp: true                 # FP16 mixed precision
  reward_mode: improved         # improved | heuristic | search
  save_freq: 1                  # Save checkpoint má»—i N epochs
  checkpoint_dir: ./checkpoints_msa_optimized

# Candidate Mining
candidate_mining:
  max_candidates: 50
  methods: [tfidf, bm25_contrib]

# RRF Fusion
rrf_fusion:
  k_constant: 60

# BERT Re-ranker
bert_reranker:
  model_name: cross-encoder/ms-marco-MiniLM-L-6-v2
  max_length: 512
  batch_size: 32
```

---

## ğŸ› Troubleshooting

### 1. CUDA Out of Memory
```bash
# Giáº£m batch size trong config
collect_batch_size: 64  # Thay vÃ¬ 128
mini_batch_size: 32     # Thay vÃ¬ 64
```

### 2. Training quÃ¡ cháº­m
```bash
# Kiá»ƒm tra GPU Ä‘Æ°á»£c dÃ¹ng
python -c "import torch; print(torch.cuda.is_available())"

# Äáº£m báº£o AMP enabled
use_amp: true  # Trong config

# Giáº£m num_query_variants
num_query_variants: 2  # Thay vÃ¬ 4
```

### 3. Java khÃ´ng tÃ¬m tháº¥y (Pyserini)
```bash
export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
```

### 4. Metrics báº±ng 0 trong training
âœ… **ÄÃ£ fix:** SimpleBM25Searcher giá» index documents tá»« Táº¤T Cáº¢ splits (train+valid+test)

### 5. Training crash sau validation
âœ… **ÄÃ£ fix:** `retrieve()` return format issue trong `evaluate()`

---

## ğŸ“ˆ Training Tips

### Reward Function Modes

**improved (Khuyáº¿n nghá»‹ - máº·c Ä‘á»‹nh):**
- Reward dá»±a trÃªn term quality (TF-IDF, BM25 scores)
- Relevance signal (terms trong relevant docs)
- Length penalty + step discount
- Stable PPO training

**heuristic (Nhanh hÆ¡n 3x, accuracy tháº¥p hÆ¡n):**
- KhÃ´ng cáº§n search engine
- Reward based on query expansion heuristics
- Tá»‘t cho prototyping

**search (Cháº­m nháº¥t, accurate nháº¥t):**
- Reward = actual Recall@100 improvement
- Cáº§n search cho má»—i action â†’ ráº¥t cháº­m

### Monitoring Training

```bash
# Xem logs realtime
tail -f checkpoints_msa_optimized/train.log

# Key metrics Ä‘á»ƒ watch:
# - avg_reward: NÃªn tÄƒng dáº§n, stable ~1.0-1.2
# - policy_loss: Negative nhá» (~-0.003 to -0.01)
# - value_loss: Giáº£m dáº§n (< 5.0)
# - cache_hit_rate: TÄƒng lÃªn ~20-40% sau vÃ i epochs
```

---

## ğŸ”¬ Ablation Studies

Äá»ƒ cháº¡y ablation studies (theo proposal):

### 1. No RL (heuristic term selection)
```python
# Trong evaluate_full.py, dÃ¹ng RM3 baseline
python evaluate_full.py --stages baseline,rm3
```

### 2. No RRF (single query only)
```yaml
# Trong config: set num_query_variants: 1
num_query_variants: 1
```

### 3. Vary m (sá»‘ query variants)
```bash
for m in 1 2 4 8 16; do
    # Edit config: num_query_variants: $m
    python evaluate_full.py --checkpoint best_model.pt
done
```

### 4. Different candidate sources
```yaml
# Chá»‰ dÃ¹ng TF-IDF
candidate_mining:
  methods: [tfidf]

# Chá»‰ dÃ¹ng BM25 contribution
candidate_mining:
  methods: [bm25_contrib]
```

### 5. Different reward functions
```yaml
training:
  reward_mode: heuristic  # hoáº·c search, improved
```

---

## ğŸ“ Citation

Náº¿u sá»­ dá»¥ng code nÃ y, vui lÃ²ng cite:

```bibtex
@misc{adaptive-ir-2026,
  title={Adaptive Information Retrieval with Deep Reinforcement Learning},
  author={Your Name},
  year={2026},
  howpublished={\url{https://github.com/your-repo}}
}
```

**Key Papers:**
- Nogueira & Cho (2017): Task-Oriented Query Reformulation with RL
- Buck et al. (2018): Term-based Query Reformulation
- Craswell et al. (2020): ORCA: Conversational search with RL
- Cormack et al. (2009): Reciprocal Rank Fusion

---

## ğŸ“ Support

- **Issues:** Má»Ÿ issue trÃªn GitHub
- **Training logs:** LÆ°u táº¡i `checkpoints_msa_optimized/train.log`
- **Checkpoint size:** ~26MB per checkpoint

---

## ğŸ“„ License

MIT License - Xem file LICENSE Ä‘á»ƒ biáº¿t chi tiáº¿t.

---

**Last updated:** January 30, 2026  
**Version:** 1.0.0  
**Status:** Production Ready âœ…
