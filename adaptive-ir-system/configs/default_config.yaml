# System Configuration
system:
  device: "cuda"  # cuda, cpu, mps
  num_threads: 20
  seed: 42
  log_level: "INFO"

# Dataset Configuration
dataset:
  name: "msmarco"  # msmarco, jeopardy, trec-car
  data_dir: "./data"
  corpus_file: "collection.tsv"
  queries_train: "queries.train.tsv"
  queries_dev: "queries.dev.small.tsv"
  qrels_train: "qrels.train.tsv"
  qrels_dev: "qrels.dev.small.tsv"
  max_train_queries: 50000
  max_eval_queries: 6980

# Retrieval Configuration
retrieval:
  engine: "pyserini"  # pyserini, bm25
  index_dir: "./data/index"
  k1: 0.9
  b: 0.4
  top_k: 100
  cache_results: true

# Candidate Mining Configuration
candidate_mining:
  enabled: true
  top_k0: 50  # Initial retrieval for mining
  max_candidates: 200
  min_candidates: 50
  methods:
    - "tfidf"      # TF-IDF scoring
    - "bm25_contrib"  # BM25 contribution
    # - "keybert"   # Optional: KeyBERT extraction
  stopwords: true
  min_term_length: 3
  max_term_length: 20

# RL Agent Configuration
rl_agent:
  enabled: true
  architecture: "actor_critic"
  
  # State encoding
  embedding_dim: 512
  state_features:
    - "query_embedding"
    - "candidate_embeddings"
    - "idf_scores"
    - "tf_scores"
    - "cosine_similarity"
    - "bm25_contribution"
  
  # Policy network
  hidden_dim: 256
  num_attention_heads: 4
  num_encoder_layers: 2
  dropout: 0.1
  
  # Training
  algorithm: "ppo"  # ppo, a2c, reinforce
  learning_rate: 3.0e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coef: 0.01
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  
  # Episodes
  max_steps_per_episode: 5
  num_query_variants: 4  # m in paper
  
  # Reward shaping
  reward:
    recall_weight: 0.7
    mrr_weight: 0.3
    length_penalty: 0.01
    metric: "recall@100"  # recall@100, mrr@10, f1

# RRF Fusion Configuration
rrf_fusion:
  enabled: true
  k_constant: 60
  normalize_scores: false

# BERT Re-ranker Configuration
bert_reranker:
  enabled: true
  model_name: "cross-encoder/ms-marco-MiniLM-L-12-v2"
  max_length: 512
  batch_size: 128
  use_fp16: true
  top_k_rerank: 100

# Training Configuration
training:
  num_epochs: 10
  batch_size: 32
  episodes_per_batch: 64
  save_freq: 1000
  eval_freq: 500
  log_freq: 100
  checkpoint_dir: "./models/checkpoints"
  best_model_path: "./models/best_model.pt"
  tensorboard_dir: "./runs"
  use_wandb: false
  wandb_project: "adaptive-ir"

# Evaluation Configuration
evaluation:
  metrics:
    - "recall@100"
    - "mrr@10"
    - "ndcg@10"
    - "map"
  measure_latency: true
  latency_runs: 100
  save_results: true
  results_dir: "./results"

# Ablation Studies
ablation:
  enabled: false
  studies:
    - "no_rl"          # Random term selection
    - "no_rrf"         # Single best query
    - "no_bert"        # BM25 scores only
    - "vary_m"         # m = [1, 2, 4, 8]
    - "vary_k"         # K = [50, 100, 200]
    - "reward_types"   # Different reward formulations

# Baselines
baselines:
  - "bm25"
  - "bm25_rm3"
  - "bm25_bert"
  # - "dense_retrieval"
