# Configuration for MS Academic Dataset (HAS CORPUS!)

system:
  device: 'cuda'
  seed: 42
  num_workers: 4

# MS Academic data (complete with corpus)
data:
  dataset_type: 'msa'  # MS Academic
  data_dir: '../Query Reformulator'
  
  # Files automatically determined:
  # dataset_file: msa_dataset.hdf5
  # corpus_file: msa_corpus.hdf5

# Legacy Word2Vec embeddings
embeddings:
  type: 'legacy'
  path: '../Query Reformulator/D_cbow_pdw_8B.pkl'
  embedding_dim: 500
  freeze: false

# Candidate Mining
candidate_mining:
  enabled: true
  max_candidates: 50
  min_score: 0.1
  methods:
    - 'tfidf'
    - 'bm25'
  top_k_per_method: 30

# RL Agent (using legacy embeddings)
rl_agent:
  enabled: true
  embedding_dim: 500  # Match legacy embeddings
  hidden_dim: 256
  num_heads: 4
  num_layers: 2
  dropout: 0.1
  max_steps_per_episode: 5
  num_query_variants: 4
  
  use_pretrained_embeddings: true
  embedding_type: 'legacy'

# RRF Fusion
rrf_fusion:
  enabled: true
  k_constant: 60
  method: 'rrf'

# BERT Re-ranker
bert_reranker:
  enabled: true
  model_name: 'cross-encoder/ms-marco-MiniLM-L-12-v2'
  batch_size: 128
  max_length: 512
  top_k_rerank: 100
  use_fp16: true

# Retrieval settings
retrieval:
  top_k: 100
  bm25_k1: 0.9
  bm25_b: 0.4

# Training
training:
  num_epochs: 50  # MS Academic is smaller
  batch_size: 32
  learning_rate: 0.0003
  episodes_per_update: 128
  ppo_epochs: 4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  
  # Reward shaping for citation recall
  reward_weights:
    recall: 0.7
    mrr: 0.3
  
  # Checkpointing
  checkpoint_dir: './checkpoints_msa'
  save_freq: 5
  
  # Early stopping
  early_stopping_patience: 10
  
  # Logging
  log_dir: './logs_msa'
  
  # Replay buffer
  buffer_size: 10000

# Evaluation metrics
evaluation:
  metrics:
    - 'recall@10'
    - 'recall@20'
    - 'recall@50'
    - 'mrr@10'
    - 'ndcg@10'
    - 'map'
    - 'precision@10'
