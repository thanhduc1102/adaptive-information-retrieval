"""
Optimized Training Loop for RL Agent

GPU-Optimized PPO training with:
- Batched episode collection (multiple queries in parallel)
- Pre-computed embeddings cache
- Vectorized reward computation
- Multi-GPU support with DataParallel
- Mixed precision training (FP16)
- Efficient memory management
- Parallel data loading

Key Optimizations:
1. BATCHED EPISODES: Process multiple queries simultaneously
2. EMBEDDING CACHE: Pre-compute all query/doc embeddings once
3. VECTORIZED OPS: Replace Python loops with tensor operations
4. GPU UTILIZATION: Keep data on GPU, minimize transfers
5. MIXED PRECISION: Use FP16 for faster computation
6. MULTI-GPU: DataParallel for 2+ GPUs
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast  # Updated API
from torch.cuda.amp import GradScaler
from torch.utils.data import DataLoader, Dataset
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
import logging
from pathlib import Path
import json
from collections import deque, defaultdict
from tqdm import tqdm
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
from dataclasses import dataclass
import hashlib

from ..rl_agent import QueryReformulatorAgent, RLTrainer
from ..pipeline import AdaptiveIRPipeline
from ..evaluation import IRMetricsAggregator
from ..utils import setup_logging, save_checkpoint, load_checkpoint, EarlyStopping


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class EpisodeData:
    """Pre-computed data for an episode."""
    query_id: str
    query: str
    query_emb: torch.Tensor
    candidate_terms: List[str]
    candidate_embs: torch.Tensor
    candidate_features: torch.Tensor
    qrels: Dict[str, int]
    num_candidates: int
    relevant_terms: Set[str] = None  # Terms from relevant documents


class EmbeddingCache:
    """
    Thread-safe cache for embeddings.
    Pre-computes and stores all embeddings to avoid redundant computation.
    Supports both legacy Word2Vec and sentence-transformers models.
    """
    
    def __init__(self, embedding_model, device: str = 'cuda', max_size: int = 100000):
        self.embedding_model = embedding_model
        self.device = device
        self.max_size = max_size
        self.cache = {}
        self.lock = threading.Lock()
        self.hits = 0
        self.misses = 0
        
        # Detect model type
        self.is_legacy = hasattr(embedding_model, 'loader') if embedding_model else False
        
    def _hash_text(self, text: str) -> str:
        """Create hash key for text."""
        return hashlib.md5(text.encode()).hexdigest()[:16]
    
    def get(self, text: str) -> torch.Tensor:
        """Get embedding from cache or compute."""
        key = self._hash_text(text)
        
        with self.lock:
            if key in self.cache:
                self.hits += 1
                return self.cache[key].clone()
        
        # Compute embedding
        if self.embedding_model is None:
            embedding = torch.randn(500)  # Default dim
        elif self.is_legacy:
            # Legacy Word2Vec adapter
            embedding = self.embedding_model.encode(text, convert_to_tensor=True)
        else:
            # Sentence-transformers
            embedding = self.embedding_model.encode(text, convert_to_tensor=True)
        
        with self.lock:
            if len(self.cache) < self.max_size:
                self.cache[key] = embedding.cpu() if isinstance(embedding, torch.Tensor) else torch.tensor(embedding)
            self.misses += 1
        
        return embedding if isinstance(embedding, torch.Tensor) else torch.tensor(embedding)
    
    def get_batch(self, texts: List[str]) -> torch.Tensor:
        """Get embeddings for multiple texts efficiently."""
        embeddings = []
        texts_to_compute = []
        indices_to_compute = []
        
        # Check cache first
        for i, text in enumerate(texts):
            key = self._hash_text(text)
            with self.lock:
                if key in self.cache:
                    embeddings.append((i, self.cache[key].clone()))
                    self.hits += 1
                else:
                    texts_to_compute.append(text)
                    indices_to_compute.append(i)
                    self.misses += 1
        
        # Batch compute missing embeddings
        if texts_to_compute:
            if self.embedding_model is None:
                # Random embeddings
                for idx, text in zip(indices_to_compute, texts_to_compute):
                    emb = torch.randn(500)
                    embeddings.append((idx, emb))
                    key = self._hash_text(text)
                    with self.lock:
                        if len(self.cache) < self.max_size:
                            self.cache[key] = emb
            elif self.is_legacy:
                # Legacy adapter - process one by one (no batch support)
                for idx, text in zip(indices_to_compute, texts_to_compute):
                    emb = self.embedding_model.encode(text, convert_to_tensor=True)
                    if not isinstance(emb, torch.Tensor):
                        emb = torch.tensor(emb)
                    embeddings.append((idx, emb.cpu()))
                    key = self._hash_text(text)
                    with self.lock:
                        if len(self.cache) < self.max_size:
                            self.cache[key] = emb.cpu()
            else:
                # Sentence-transformers - true batch encoding
                computed = self.embedding_model.encode(
                    texts_to_compute, 
                    convert_to_tensor=True,
                    batch_size=128,
                    show_progress_bar=False
                )
                
                # Add to results and cache
                for i, (idx, text) in enumerate(zip(indices_to_compute, texts_to_compute)):
                    emb = computed[i] if len(computed.shape) > 1 else computed
                    embeddings.append((idx, emb.cpu()))
                    key = self._hash_text(text)
                    with self.lock:
                        if len(self.cache) < self.max_size:
                            self.cache[key] = emb.cpu()
        
        # Sort by original index and stack
        embeddings.sort(key=lambda x: x[0])
        return torch.stack([e[1] for e in embeddings])
    
    def precompute_queries(self, queries: Dict[str, str]):
        """Pre-compute all query embeddings."""
        texts = list(queries.values())
        # Process in smaller batches for progress visibility
        batch_size = 1000
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            self.get_batch(batch)
    
    def stats(self) -> Dict[str, float]:
        """Return cache statistics."""
        total = self.hits + self.misses
        return {
            'size': len(self.cache),
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': self.hits / total if total > 0 else 0
        }


class OptimizedReplayBuffer:
    """
    GPU-optimized replay buffer with pre-allocated tensors.
    """
    
    def __init__(
        self,
        capacity: int,
        embedding_dim: int,
        max_candidates: int,
        num_features: int,
        device: str = 'cuda'
    ):
        self.capacity = capacity
        self.device = device
        self.embedding_dim = embedding_dim
        self.max_candidates = max_candidates
        self.num_features = num_features
        
        # Pre-allocate tensors on GPU
        self.query_embs = torch.zeros(capacity, embedding_dim, device=device)
        self.current_query_embs = torch.zeros(capacity, embedding_dim, device=device)
        self.candidate_embs = torch.zeros(capacity, max_candidates, embedding_dim, device=device)
        self.candidate_features = torch.zeros(capacity, max_candidates, num_features, device=device)
        self.candidate_masks = torch.zeros(capacity, max_candidates, dtype=torch.bool, device=device)
        self.actions = torch.zeros(capacity, dtype=torch.long, device=device)
        self.log_probs = torch.zeros(capacity, device=device)
        self.values = torch.zeros(capacity, device=device)
        self.rewards = torch.zeros(capacity, device=device)
        self.dones = torch.zeros(capacity, dtype=torch.bool, device=device)
        
        self.position = 0
        self.size = 0
    
    def push(
        self,
        query_emb: torch.Tensor,
        current_query_emb: torch.Tensor,
        candidate_embs: torch.Tensor,
        candidate_features: torch.Tensor,
        candidate_mask: torch.Tensor,
        action: int,
        log_prob: float,
        value: float,
        reward: float,
        done: bool
    ):
        """Add experience to buffer."""
        idx = self.position
        num_cands = candidate_embs.shape[0]
        
        self.query_embs[idx] = query_emb
        self.current_query_embs[idx] = current_query_emb
        self.candidate_embs[idx, :num_cands] = candidate_embs
        self.candidate_features[idx, :num_cands] = candidate_features
        self.candidate_masks[idx] = False
        self.candidate_masks[idx, :num_cands] = True
        self.actions[idx] = action
        self.log_probs[idx] = log_prob
        self.values[idx] = value
        self.rewards[idx] = reward
        self.dones[idx] = done
        
        self.position = (self.position + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def push_batch(
        self,
        query_embs: torch.Tensor,
        current_query_embs: torch.Tensor,
        candidate_embs: torch.Tensor,
        candidate_features: torch.Tensor,
        candidate_masks: torch.Tensor,
        actions: torch.Tensor,
        log_probs: torch.Tensor,
        values: torch.Tensor,
        rewards: torch.Tensor,
        dones: torch.Tensor
    ):
        """Add batch of experiences."""
        batch_size = query_embs.shape[0]
        
        for i in range(batch_size):
            num_cands = candidate_masks[i].sum().item()
            self.push(
                query_embs[i],
                current_query_embs[i],
                candidate_embs[i, :num_cands],
                candidate_features[i, :num_cands],
                candidate_masks[i],
                actions[i].item(),
                log_probs[i].item(),
                values[i].item(),
                rewards[i].item(),
                dones[i].item()
            )
    
    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:
        """Sample batch of experiences."""
        indices = torch.randint(0, self.size, (batch_size,), device=self.device)
        
        return {
            'query_emb': self.query_embs[indices],
            'current_query_emb': self.current_query_embs[indices],
            'candidate_embs': self.candidate_embs[indices],
            'candidate_features': self.candidate_features[indices],
            'candidate_mask': self.candidate_masks[indices],
            'actions': self.actions[indices],
            'old_log_probs': self.log_probs[indices],
            'values': self.values[indices],
            'rewards': self.rewards[indices],
            'dones': self.dones[indices]
        }
    
    def get_all(self) -> Dict[str, torch.Tensor]:
        """Get all data in buffer."""
        return {
            'query_emb': self.query_embs[:self.size],
            'current_query_emb': self.current_query_embs[:self.size],
            'candidate_embs': self.candidate_embs[:self.size],
            'candidate_features': self.candidate_features[:self.size],
            'candidate_mask': self.candidate_masks[:self.size],
            'actions': self.actions[:self.size],
            'old_log_probs': self.log_probs[:self.size],
            'values': self.values[:self.size],
            'rewards': self.rewards[:self.size],
            'dones': self.dones[:self.size]
        }
    
    def clear(self):
        """Clear buffer."""
        self.position = 0
        self.size = 0
    
    def __len__(self):
        return self.size


# ============================================================================
# OPTIMIZED TRAINER
# ============================================================================

class OptimizedRLTrainer:
    """
    GPU-optimized PPO Trainer with mixed precision.
    """
    
    def __init__(
        self,
        agent: QueryReformulatorAgent,
        config: Dict,
        device: str = 'cuda',
        use_amp: bool = True,
        multi_gpu: bool = False
    ):
        self.device = device
        self.use_amp = use_amp and torch.cuda.is_available()
        
        # Wrap for multi-GPU if available
        if multi_gpu and torch.cuda.device_count() > 1:
            print(f"Using {torch.cuda.device_count()} GPUs!")
            self.agent = nn.DataParallel(agent)
            self.agent_module = agent  # Keep reference to original
        else:
            self.agent = agent
            self.agent_module = agent
        
        self.agent = self.agent.to(device)
        
        # Hyperparameters
        self.lr = config.get('learning_rate', 3e-4)
        self.gamma = config.get('gamma', 0.99)
        self.gae_lambda = config.get('gae_lambda', 0.95)
        self.clip_epsilon = config.get('clip_epsilon', 0.2)
        self.entropy_coef = config.get('entropy_coef', 0.01)
        self.value_loss_coef = config.get('value_loss_coef', 0.5)
        self.max_grad_norm = config.get('max_grad_norm', 0.5)
        self.num_epochs = config.get('num_ppo_epochs', 4)
        self.mini_batch_size = config.get('mini_batch_size', 64)
        
        # Optimizer
        self.optimizer = torch.optim.AdamW(
            self.agent_module.parameters(),
            lr=self.lr,
            eps=1e-5
        )
        
        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=1000,
            eta_min=1e-6
        )
        
        # Mixed precision scaler (use new API)
        self.scaler = torch.amp.GradScaler('cuda') if self.use_amp else None
    
    @torch.no_grad()
    def compute_gae_vectorized(
        self,
        rewards: torch.Tensor,
        values: torch.Tensor,
        dones: torch.Tensor,
        next_value: float = 0.0
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Vectorized GAE computation.
        """
        batch_size = rewards.shape[0]
        advantages = torch.zeros_like(rewards)
        
        gae = 0
        for t in reversed(range(batch_size)):
            if t == batch_size - 1:
                next_val = next_value
                next_non_terminal = 1.0 - dones[t].float()
            else:
                next_val = values[t + 1]
                next_non_terminal = 1.0 - dones[t].float()
            
            delta = rewards[t] + self.gamma * next_val * next_non_terminal - values[t]
            gae = delta + self.gamma * self.gae_lambda * next_non_terminal * gae
            advantages[t] = gae
        
        returns = advantages + values
        return advantages, returns
    
    def update(self, rollout_buffer: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        PPO update with mini-batching and mixed precision.
        """
        # Get data
        query_emb = rollout_buffer['query_emb']
        current_query_emb = rollout_buffer['current_query_emb']
        candidate_embs = rollout_buffer['candidate_embs']
        candidate_features = rollout_buffer['candidate_features']
        candidate_mask = rollout_buffer['candidate_mask']
        actions = rollout_buffer['actions']
        old_log_probs = rollout_buffer['old_log_probs']
        rewards = rollout_buffer['rewards']
        dones = rollout_buffer['dones']
        values = rollout_buffer['values']
        
        # Compute advantages
        advantages, returns = self.compute_gae_vectorized(rewards, values, dones)
        
        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # Track metrics
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy = 0
        num_updates = 0
        
        batch_size = query_emb.shape[0]
        indices = np.arange(batch_size)
        
        for epoch in range(self.num_epochs):
            np.random.shuffle(indices)
            
            for start in range(0, batch_size, self.mini_batch_size):
                end = start + self.mini_batch_size
                mb_indices = indices[start:end]
                
                mb_query_emb = query_emb[mb_indices]
                mb_current_query_emb = current_query_emb[mb_indices]
                mb_candidate_embs = candidate_embs[mb_indices]
                mb_candidate_features = candidate_features[mb_indices]
                mb_candidate_mask = candidate_mask[mb_indices]
                mb_actions = actions[mb_indices]
                mb_old_log_probs = old_log_probs[mb_indices]
                mb_advantages = advantages[mb_indices]
                mb_returns = returns[mb_indices]
                
                # Forward pass with mixed precision (use 'cuda' device_type for new API)
                with autocast('cuda', enabled=self.use_amp):
                    log_probs, new_values, entropy = self.agent_module.evaluate_actions(
                        mb_query_emb,
                        mb_current_query_emb,
                        mb_candidate_embs,
                        mb_candidate_features,
                        mb_actions,
                        mb_candidate_mask
                    )
                    
                    # PPO loss
                    ratio = torch.exp(log_probs - mb_old_log_probs)
                    surr1 = ratio * mb_advantages
                    surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_advantages
                    policy_loss = -torch.min(surr1, surr2).mean()
                    
                    # Value loss
                    value_loss = F.mse_loss(new_values.squeeze(), mb_returns)
                    
                    # Entropy bonus
                    entropy_loss = -entropy.mean()
                    
                    # Total loss
                    loss = policy_loss + self.value_loss_coef * value_loss + self.entropy_coef * entropy_loss
                
                # Backward pass
                self.optimizer.zero_grad()
                
                if self.use_amp:
                    self.scaler.scale(loss).backward()
                    self.scaler.unscale_(self.optimizer)
                    nn.utils.clip_grad_norm_(self.agent_module.parameters(), self.max_grad_norm)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    nn.utils.clip_grad_norm_(self.agent_module.parameters(), self.max_grad_norm)
                    self.optimizer.step()
                
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy += entropy.mean().item()
                num_updates += 1
        
        self.scheduler.step()
        
        return {
            'policy_loss': total_policy_loss / num_updates,
            'value_loss': total_value_loss / num_updates,
            'entropy': total_entropy / num_updates,
            'lr': self.optimizer.param_groups[0]['lr']
        }


# ============================================================================
# BATCHED EPISODE COLLECTOR
# ============================================================================

class BatchedEpisodeCollector:
    """
    Collects episodes in batches for GPU efficiency.
    """
    
    def __init__(
        self,
        pipeline: AdaptiveIRPipeline,
        embedding_cache: EmbeddingCache,
        device: str = 'cuda',
        max_candidates: int = 50,
        max_steps: int = 5,
        dataset = None  # Reference to dataset for relevant term extraction
    ):
        self.pipeline = pipeline
        self.embedding_cache = embedding_cache
        self.device = device
        self.max_candidates = max_candidates
        self.max_steps = max_steps
        self.dataset = dataset
        
        # Cache for relevant terms per query (to avoid recomputing)
        self.relevant_terms_cache = {}
        
        self.logger = logging.getLogger(__name__)
    
    def _extract_relevant_terms(self, qrels: Dict[str, int]) -> Set[str]:
        """
        Extract unique terms from relevant documents.
        
        Args:
            qrels: {doc_id: relevance} mapping
            
        Returns:
            Set of terms appearing in relevant documents
        """
        if self.dataset is None:
            return set()
        
        terms = set()
        for doc_id in qrels.keys():
            try:
                doc_text = self.dataset.get_document(doc_id)
                if doc_text and not doc_text.startswith("Document ID:"):
                    # Tokenize and add terms
                    tokens = doc_text.lower().split()
                    # Filter: only alphanumeric, length 2-20
                    tokens = [t for t in tokens if t.isalnum() and 2 <= len(t) <= 20]
                    terms.update(tokens)
            except Exception:
                continue
        
        return terms
    
    def prepare_episode_data(
        self,
        query_id: str,
        query: str,
        qrels: Dict[str, int]
    ) -> Optional[EpisodeData]:
        """
        Pre-compute all data needed for an episode.
        This can be done in parallel.
        """
        # Mine candidates
        candidates = self.pipeline.mine_candidates(query)
        
        if not candidates:
            return None
        
        # Get embeddings
        query_emb = self.embedding_cache.get(query)
        
        candidate_terms = list(candidates.keys())[:self.max_candidates]
        candidate_embs = self.embedding_cache.get_batch(candidate_terms)
        
        # Get features
        feature_matrix = self.pipeline.candidate_miner.get_candidate_features(
            {k: candidates[k] for k in candidate_terms}
        )
        candidate_features = torch.tensor(feature_matrix, dtype=torch.float32)
        
        # Extract relevant terms from documents (for improved reward)
        # Use cache to avoid recomputing for same qrels
        qrels_key = tuple(sorted(qrels.keys()))
        if qrels_key in self.relevant_terms_cache:
            relevant_terms = self.relevant_terms_cache[qrels_key]
        else:
            relevant_terms = self._extract_relevant_terms(qrels)
            # Cache only if not too large (limit cache size)
            if len(self.relevant_terms_cache) < 50000:
                self.relevant_terms_cache[qrels_key] = relevant_terms
        
        return EpisodeData(
            query_id=query_id,
            query=query,
            query_emb=query_emb,
            candidate_terms=candidate_terms,
            candidate_embs=candidate_embs,
            candidate_features=candidate_features,
            qrels=qrels,
            num_candidates=len(candidate_terms),
            relevant_terms=relevant_terms
        )
    
    def prepare_batch_parallel(
        self,
        queries: Dict[str, str],
        qrels_all: Dict[str, Dict[str, int]],
        num_workers: int = 4
    ) -> List[EpisodeData]:
        """
        Prepare episode data in parallel.
        """
        episode_data_list = []
        
        # Use ThreadPoolExecutor for I/O-bound preparation
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = []
            for query_id, query in queries.items():
                qrels = qrels_all.get(query_id, {})
                if qrels:
                    futures.append(
                        executor.submit(self.prepare_episode_data, query_id, query, qrels)
                    )
            
            for future in futures:
                result = future.result()
                if result is not None:
                    episode_data_list.append(result)
        
        return episode_data_list
    
    @torch.no_grad()
    def collect_batch_episodes(
        self,
        episode_data_list: List[EpisodeData],
        batch_size: int,
        reward_fn
    ) -> Tuple[List[Dict], List[float]]:
        """
        Collect episodes in batches using vectorized operations.
        
        Returns:
            experiences: List of experience dictionaries
            rewards: Total rewards for each episode
        """
        all_experiences = []
        all_rewards = []
        
        # Process in batches
        for batch_start in range(0, len(episode_data_list), batch_size):
            batch_end = min(batch_start + batch_size, len(episode_data_list))
            batch_data = episode_data_list[batch_start:batch_end]
            actual_batch_size = len(batch_data)
            
            # Pad and stack tensors
            batch_query_embs = torch.stack([d.query_emb for d in batch_data]).to(self.device)
            
            # Handle variable-length candidates with padding
            max_cands = max(d.num_candidates for d in batch_data)
            emb_dim = batch_data[0].candidate_embs.shape[1]
            feat_dim = batch_data[0].candidate_features.shape[1]
            
            batch_candidate_embs = torch.zeros(actual_batch_size, max_cands, emb_dim, device=self.device)
            batch_candidate_features = torch.zeros(actual_batch_size, max_cands, feat_dim, device=self.device)
            batch_candidate_masks = torch.zeros(actual_batch_size, max_cands, dtype=torch.bool, device=self.device)
            
            for i, data in enumerate(batch_data):
                n = data.num_candidates
                batch_candidate_embs[i, :n] = data.candidate_embs.to(self.device)
                batch_candidate_features[i, :n] = data.candidate_features.to(self.device)
                batch_candidate_masks[i, :n] = True
            
            # Track current queries for each item in batch
            current_queries = [data.query for data in batch_data]
            current_query_embs = batch_query_embs.clone()
            
            # Episode rewards
            batch_rewards = [0.0] * actual_batch_size
            active_mask = [True] * actual_batch_size  # Which episodes are still active
            
            # Run episodes step by step
            for step in range(self.max_steps):
                if not any(active_mask):
                    break
                
                # Get actions for all active episodes in batch
                # Disable AMP during episode collection to avoid FP16 overflow in masking
                with torch.inference_mode(False):
                    actions, log_probs, values = self.pipeline.rl_agent.select_action(
                        batch_query_embs,
                        current_query_embs,
                        batch_candidate_embs,
                        batch_candidate_features,
                        batch_candidate_masks,
                        deterministic=False
                    )
                
                # Process each episode in batch
                for i in range(actual_batch_size):
                    if not active_mask[i]:
                        continue
                    
                    data = batch_data[i]
                    action_idx = actions[i].item()
                    
                    # Check if STOP action
                    done = (action_idx >= data.num_candidates)
                    
                    selected_term = None
                    candidate_features_dict = None
                    
                    if not done:
                        # Add selected term
                        selected_term = data.candidate_terms[action_idx]
                        current_queries[i] = current_queries[i] + " " + selected_term
                        
                        # Get candidate features for reward computation
                        feat_tensor = data.candidate_features[action_idx]
                        candidate_features_dict = {
                            'tfidf': feat_tensor[0].item() if feat_tensor.dim() > 0 else 0.0,
                            'bm25_contrib': feat_tensor[1].item() if feat_tensor.dim() > 0 and feat_tensor.shape[0] > 1 else 0.0,
                        }
                    
                    # Compute reward using reward_fn
                    try:
                        # Try improved reward first (9 args with relevant_terms)
                        reward = reward_fn(
                            data.query,
                            current_queries[i],
                            data.qrels,
                            action_idx,
                            done,
                            selected_term,
                            candidate_features_dict,
                            step,
                            data.relevant_terms  # Pass relevant terms for relevance signal
                        )
                    except TypeError:
                        try:
                            # Try without relevant_terms (8 args)
                            reward = reward_fn(
                                data.query,
                                current_queries[i],
                                data.qrels,
                                action_idx,
                                done,
                                selected_term,
                                candidate_features_dict,
                                step
                            )
                        except TypeError:
                            try:
                                # Try heuristic reward (5 args)
                                reward = reward_fn(
                                    data.query,
                                    current_queries[i],
                                    data.qrels,
                                    action_idx,
                                    done
                                )
                            except TypeError:
                                # Fall back to cached reward (3 args)
                                reward = reward_fn(
                                    data.query,
                                    current_queries[i],
                                    data.qrels
                                )
                    batch_rewards[i] += reward
                    
                    # Store experience
                    all_experiences.append({
                        'query_emb': batch_query_embs[i].cpu(),
                        'current_query_emb': current_query_embs[i].cpu(),
                        'candidate_embs': batch_candidate_embs[i, :data.num_candidates].cpu(),
                        'candidate_features': batch_candidate_features[i, :data.num_candidates].cpu(),
                        'action': action_idx,
                        'log_prob': log_probs[i].item(),
                        'value': values[i].item(),
                        'reward': reward,
                        'done': done
                    })
                    
                    if done:
                        active_mask[i] = False
                    else:
                        # Update current query embedding
                        current_query_embs[i] = self.embedding_cache.get(current_queries[i]).to(self.device)
            
            all_rewards.extend(batch_rewards)
        
        return all_experiences, all_rewards


# ============================================================================
# MAIN TRAINING LOOP
# ============================================================================

class OptimizedRLTrainingLoop:
    """
    GPU-optimized training loop for RL query reformulation.
    """
    
    def __init__(
        self,
        config: Dict,
        pipeline: AdaptiveIRPipeline,
        train_dataset,
        val_dataset,
        test_dataset=None
    ):
        self.config = config
        self.pipeline = pipeline
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.test_dataset = test_dataset
        
        # Device setup
        self.device = config.get('system', {}).get('device', 'cuda')
        self.multi_gpu = torch.cuda.device_count() > 1
        
        if self.multi_gpu:
            print(f"üöÄ Multi-GPU training enabled: {torch.cuda.device_count()} GPUs")
        
        # Training parameters
        self.num_epochs = config.get('training', {}).get('num_epochs', 100)
        self.batch_size = config.get('training', {}).get('batch_size', 32)
        self.collect_batch_size = config.get('training', {}).get('collect_batch_size', 16)
        self.episodes_per_update = config.get('training', {}).get('episodes_per_update', 256)
        self.ppo_epochs = config.get('training', {}).get('ppo_epochs', 4)
        self.max_steps = config.get('rl_agent', {}).get('max_steps_per_episode', 5)
        self.max_candidates = config.get('candidate_mining', {}).get('max_candidates', 50)
        
        # Mixed precision
        self.use_amp = config.get('training', {}).get('use_amp', True)
        
        # Reward shaping
        self.reward_weights = config.get('training', {}).get('reward_weights', {
            'recall': 0.7,
            'mrr': 0.3
        })
        
        # Initialize embedding cache
        self.embedding_cache = EmbeddingCache(
            pipeline.embedding_model,
            device=self.device,
            max_size=200000
        )
        
        # Optimized RL Trainer
        self.rl_trainer = OptimizedRLTrainer(
            agent=pipeline.rl_agent,
            config=config['rl_agent'],
            device=self.device,
            use_amp=self.use_amp,
            multi_gpu=self.multi_gpu
        )
        
        # Optimized replay buffer
        embedding_dim = config.get('rl_agent', {}).get('embedding_dim', 500)
        num_features = 3  # tfidf, bm25_contrib, keybert
        buffer_size = config.get('training', {}).get('buffer_size', 50000)
        
        self.replay_buffer = OptimizedReplayBuffer(
            capacity=buffer_size,
            embedding_dim=embedding_dim,
            max_candidates=self.max_candidates,
            num_features=num_features,
            device=self.device
        )
        
        # Episode collector
        self.collector = BatchedEpisodeCollector(
            pipeline=pipeline,
            embedding_cache=self.embedding_cache,
            device=self.device,
            max_candidates=self.max_candidates,
            max_steps=self.max_steps,
            dataset=train_dataset  # Pass dataset for relevant term extraction
        )
        
        # Checkpointing
        self.checkpoint_dir = Path(config.get('training', {}).get('checkpoint_dir', './checkpoints'))
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.save_freq = config.get('training', {}).get('save_freq', 5)
        
        # Early stopping
        patience = config.get('training', {}).get('early_stopping_patience', 10)
        self.early_stopping = EarlyStopping(patience=patience, mode='max')
        
        # Logging
        self.logger = logging.getLogger(__name__)
        self.metrics_history = defaultdict(list)
        
        # Search result cache for reward computation
        self.search_cache = {}
        
        self.logger.info("=" * 60)
        self.logger.info("Optimized RL Training Loop Initialized")
        self.logger.info(f"  Device: {self.device}")
        self.logger.info(f"  Multi-GPU: {self.multi_gpu}")
        self.logger.info(f"  Mixed Precision (AMP): {self.use_amp}")
        self.logger.info(f"  Batch Size: {self.batch_size}")
        self.logger.info(f"  Collect Batch Size: {self.collect_batch_size}")
        self.logger.info(f"  Episodes per Update: {self.episodes_per_update}")
        self.logger.info(f"  Buffer Size: {buffer_size}")
        self.logger.info("=" * 60)
        
        # Reward mode: 'improved' (recommended), 'heuristic' (fast), 'search' (slow but accurate)
        self.reward_mode = config.get('training', {}).get('reward_mode', 'improved')
        self.use_heuristic_reward = config.get('training', {}).get('use_heuristic_reward', False)  # Deprecated
        
        # Precompute document texts for qrels (for relevance signal)
        self.doc_texts_cache = {}
        self._precompute_doc_cache_enabled = config.get('training', {}).get('precompute_doc_cache', False)
        
        # HuggingFace upload config
        self.hf_config = config.get('huggingface', {})
        self.hf_enabled = self.hf_config.get('enabled', False)
        self.hf_repo_id = self.hf_config.get('repo_id', None)
        self.hf_token = self.hf_config.get('token', None)
        self.hf_uploader = None
        
        if self.hf_enabled and self.hf_repo_id:
            try:
                from ..utils.huggingface_uploader import HuggingFaceUploader
                self.hf_uploader = HuggingFaceUploader(
                    repo_id=self.hf_repo_id,
                    token=self.hf_token,
                    private=self.hf_config.get('private', False)
                )
                self.logger.info(f"üîó HuggingFace upload enabled: {self.hf_repo_id}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è HuggingFace upload disabled: {e}")
    
    def compute_heuristic_reward(
        self,
        original_query: str,
        current_query: str,
        qrels: Dict[str, int],
        action_idx: int,
        done: bool
    ) -> float:
        """
        Fast heuristic reward without search.
        This speeds up episode collection dramatically.
        
        Heuristics:
        1. Adding relevant terms (found in relevant docs) ‚Üí positive reward
        2. Short reformulated query ‚Üí penalty
        3. Too long query ‚Üí penalty
        4. Stop action when query improved ‚Üí bonus
        """
        # Quick heuristic based on query expansion
        original_tokens = set(original_query.lower().split())
        current_tokens = set(current_query.lower().split())
        added_tokens = current_tokens - original_tokens
        
        reward = 0.0
        
        if done:
            # Stop action - give small reward for making decision
            if len(added_tokens) > 0:
                reward = 0.1 * len(added_tokens)  # Bonus for expanding
            else:
                reward = -0.1  # Penalty for stopping without expansion
        else:
            # Term selection - reward based on term quality
            # More terms = good (up to a point)
            if len(added_tokens) <= 3:
                reward = 0.05  # Small positive reward
            else:
                reward = -0.02  # Penalty for too many terms
        
        return reward
    
    def compute_improved_reward(
        self,
        original_query: str,
        current_query: str,
        qrels: Dict[str, int],
        action_idx: int,
        done: bool,
        selected_term: str = None,
        candidate_features: Dict[str, float] = None,
        step: int = 0,
        relevant_terms: Set[str] = None
    ) -> float:
        """
        Improved reward function v·ªõi multiple signals:
        1. Term quality signal (t·ª´ candidate features - TF-IDF, BM25)
        2. Relevance signal (term xu·∫•t hi·ªán trong relevant docs) - NEW
        3. Query length management
        4. Step-based shaping
        5. Done bonus/penalty
        
        C·∫£i ti·∫øn cho PPO:
        - Reward ƒë∆∞·ª£c normalize v·ªÅ [-1, 1] ƒë·ªÉ stable training
        - Step discount ƒë·ªÉ khuy·∫øn kh√≠ch ch·ªçn terms t·ªët s·ªõm
        - Relevance signal gi√∫p agent h·ªçc terms th·ª±c s·ª± h·ªØu √≠ch
        
        Args:
            relevant_terms: Set c√°c terms xu·∫•t hi·ªán trong relevant documents
        
        Returns:
            Scalar reward trong kho·∫£ng [-1, 1]
        """
        reward = 0.0
        
        original_tokens = set(original_query.lower().split())
        current_tokens = set(current_query.lower().split())
        added_tokens = current_tokens - original_tokens
        num_added = len(added_tokens)
        
        if not done:
            # === TERM SELECTION REWARD ===
            
            # 1. Relevance Signal (quan tr·ªçng nh·∫•t)
            # N·∫øu term xu·∫•t hi·ªán trong relevant docs -> higher reward
            if selected_term and relevant_terms:
                if selected_term.lower() in relevant_terms:
                    reward += 0.35  # Strong signal cho relevant term
                else:
                    reward += 0.05  # Small reward cho expansion
            
            # 2. Term Quality Signal (s·ª≠ d·ª•ng features ƒë√£ compute)
            if candidate_features is not None:
                tfidf_score = candidate_features.get('tfidf', 0.0)
                bm25_score = candidate_features.get('bm25_contrib', 0.0)
                # Normalize v√† combine - scale to [0, 0.3]
                # S·ª≠ d·ª•ng min ƒë·ªÉ tr√°nh outliers
                term_quality = 0.6 * min(tfidf_score, 1.0) + 0.4 * min(bm25_score, 1.0)
                reward += 0.3 * term_quality
            else:
                # Fallback: small positive reward for selecting any term
                reward += 0.05
            
            # 3. Diversity bonus - th∆∞·ªüng terms m·ªõi kh√¥ng c√≥ trong query g·ªëc
            if selected_term and selected_term.lower() not in original_tokens:
                reward += 0.1
            
            # 4. Length penalty - ph·∫°t query qu√° d√†i (> 4 terms added)
            if num_added > 4:
                reward -= 0.1 * (num_added - 4)
            
            # 5. Step discount - khuy·∫øn kh√≠ch ch·ªçn terms t·ªët s·ªõm
            # Gi·∫£m d·∫ßn reward theo step ƒë·ªÉ agent h·ªçc priority
            step_discount = 0.95 ** step
            reward *= step_discount
            
        else:
            # === STOP ACTION REWARD ===
            
            # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng query cu·ªëi c√πng
            if num_added == 0:
                # Penalty cho stopping m√† kh√¥ng expand
                reward = -0.4
            elif 1 <= num_added <= 3:
                # Optimal range - bonus
                # Ki·ªÉm tra xem c√°c terms added c√≥ relevant kh√¥ng
                if relevant_terms:
                    relevant_added = sum(1 for t in added_tokens if t.lower() in relevant_terms)
                    if relevant_added > 0:
                        reward = 0.4 + 0.15 * relevant_added  # Bonus cho relevant terms
                    else:
                        reward = 0.2  # Base reward cho expansion h·ª£p l√Ω
                else:
                    reward = 0.3 + 0.1 * num_added
            elif 4 <= num_added <= 5:
                # Acceptable range
                reward = 0.15
            else:
                # Too many terms - penalty
                reward = 0.05 - 0.05 * (num_added - 5)
        
        # Clamp reward to [-1, 1] ƒë·ªÉ stable PPO training
        return max(-1.0, min(1.0, reward))
    
    def compute_reward_cached(
        self,
        original_query: str,
        current_query: str,
        qrels: Dict[str, int]
    ) -> float:
        """
        Compute reward with caching for repeated queries.
        """
        # Create cache key
        cache_key = (original_query, current_query, frozenset(qrels.keys()))
        
        if cache_key in self.search_cache:
            metrics_before, metrics_after = self.search_cache[cache_key]
        else:
            # Search for original query
            result_before = self.pipeline.search(original_query, top_k=100)
            doc_ids_before = [doc_id for doc_id, _ in result_before['results']]
            
            evaluator = IRMetricsAggregator()
            metrics_before = evaluator.compute_single_query(qrels, doc_ids_before, None)
            
            # Search for current query
            result_after = self.pipeline.search(current_query, top_k=100)
            doc_ids_after = [doc_id for doc_id, _ in result_after['results']]
            
            metrics_after = evaluator.compute_single_query(qrels, doc_ids_after, None)
            
            # Cache results
            self.search_cache[cache_key] = (metrics_before, metrics_after)
        
        # Compute reward
        delta_recall = metrics_after.get('recall@100', 0.0) - metrics_before.get('recall@100', 0.0)
        delta_mrr = metrics_after.get('mrr', 0.0) - metrics_before.get('mrr', 0.0)
        
        return (
            self.reward_weights['recall'] * delta_recall +
            self.reward_weights['mrr'] * delta_mrr
        )
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """
        Train for one epoch with batched collection.
        """
        self.pipeline.enable_training_mode()
        
        epoch_rewards = []
        epoch_losses = []
        episode_count = 0
        
        # Load data
        train_queries = self.train_dataset.load_queries()
        train_qrels = self.train_dataset.load_qrels()
        
        # Pre-compute query embeddings
        self.logger.info("Pre-computing query embeddings...")
        self.embedding_cache.precompute_queries(train_queries)
        
        # Filter queries with qrels
        valid_queries = {
            qid: q for qid, q in train_queries.items()
            if qid in train_qrels and train_qrels[qid]
        }
        
        query_ids = list(valid_queries.keys())
        np.random.shuffle(query_ids)
        
        # Process in batches
        pbar = tqdm(
            range(0, len(query_ids), self.collect_batch_size),
            desc=f"Epoch {epoch+1}/{self.num_epochs}"
        )
        
        # Choose reward function based on config
        if self.reward_mode == 'improved':
            reward_fn = self.compute_improved_reward
            self.logger.info("Using improved reward (term quality + shaping)")
        elif self.reward_mode == 'heuristic' or self.use_heuristic_reward:
            reward_fn = self.compute_heuristic_reward
            self.logger.info("Using heuristic reward (fast mode)")
        else:
            reward_fn = self.compute_reward_cached
            self.logger.info("Using search-based reward (slow but accurate)")
        
        for batch_start in pbar:
            batch_end = min(batch_start + self.collect_batch_size, len(query_ids))
            batch_query_ids = query_ids[batch_start:batch_end]
            
            # Prepare episode data
            batch_queries = {qid: valid_queries[qid] for qid in batch_query_ids}
            batch_qrels = {qid: train_qrels[qid] for qid in batch_query_ids}
            
            episode_data_list = self.collector.prepare_batch_parallel(
                batch_queries, batch_qrels, num_workers=4
            )
            
            if not episode_data_list:
                continue
            
            # Collect episodes
            experiences, rewards = self.collector.collect_batch_episodes(
                episode_data_list,
                batch_size=self.batch_size,
                reward_fn=reward_fn
            )
            
            # Add to buffer
            for exp in experiences:
                # Ensure tensors are on correct device
                self.replay_buffer.push(
                    query_emb=exp['query_emb'].to(self.device),
                    current_query_emb=exp['current_query_emb'].to(self.device),
                    candidate_embs=exp['candidate_embs'].to(self.device),
                    candidate_features=exp['candidate_features'].to(self.device),
                    candidate_mask=torch.ones(exp['candidate_embs'].shape[0], dtype=torch.bool, device=self.device),
                    action=exp['action'],
                    log_prob=exp['log_prob'],
                    value=exp['value'],
                    reward=exp['reward'],
                    done=exp['done']
                )
            
            epoch_rewards.extend(rewards)
            episode_count += len(rewards)
            
            # Update policy
            if len(self.replay_buffer) >= self.episodes_per_update:
                for _ in range(self.ppo_epochs):
                    rollout_data = self.replay_buffer.sample(min(len(self.replay_buffer), self.batch_size * 8))
                    loss_metrics = self.rl_trainer.update(rollout_data)
                    epoch_losses.append(loss_metrics)
                
                # Clear old experiences
                if len(self.replay_buffer) > self.replay_buffer.capacity * 0.8:
                    self.replay_buffer.clear()
            
            # Update progress bar
            avg_reward = np.mean(epoch_rewards[-100:]) if epoch_rewards else 0
            pbar.set_postfix({
                'reward': f"{avg_reward:.4f}",
                'episodes': episode_count,
                'buffer': len(self.replay_buffer),
                'cache_hit': f"{self.embedding_cache.stats()['hit_rate']:.2%}"
            })
        
        # Clear search cache periodically
        if len(self.search_cache) > 10000:
            self.search_cache.clear()
        
        # Aggregate metrics
        metrics = {
            'avg_reward': np.mean(epoch_rewards) if epoch_rewards else 0.0,
            'num_episodes': episode_count,
            'buffer_size': len(self.replay_buffer),
            'embedding_cache_hit_rate': self.embedding_cache.stats()['hit_rate']
        }
        
        if epoch_losses:
            for key in epoch_losses[0].keys():
                values = [loss[key] for loss in epoch_losses]
                metrics[f'avg_{key}'] = np.mean(values)
        
        return metrics
    
    def evaluate(self, dataset, split: str = 'val') -> Dict[str, float]:
        """
        Evaluate on validation/test set.
        """
        self.pipeline.enable_eval_mode()
        
        queries = dataset.load_queries()
        qrels = dataset.load_qrels()
        
        evaluator = IRMetricsAggregator()
        
        for query_id, query in tqdm(queries.items(), desc=f"Evaluating {split}"):
            qrel = qrels.get(query_id, {})
            if not qrel:
                continue
            
            result = self.pipeline.search(query, top_k=100)
            doc_ids = [doc_id for doc_id, _ in result['results']]
            
            relevant_set = set(qrel.keys())
            evaluator.add_query_result(
                query_id=query_id,
                retrieved=doc_ids,
                relevant=relevant_set,
                relevant_grades=qrel
            )
        
        return evaluator.compute_aggregate()
    
    def train(self):
        """
        Main training loop.
        """
        self.logger.info("Starting optimized training...")
        
        # Print GPU info
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                self.logger.info(f"GPU {i}: {props.name}, {props.total_memory / 1e9:.1f} GB")
        
        best_metric = 0.0
        
        for epoch in range(self.num_epochs):
            start_time = time.time()
            
            # Train epoch
            train_metrics = self.train_epoch(epoch)
            
            epoch_time = time.time() - start_time
            
            # Log metrics
            loss_str = ""
            if 'avg_policy_loss' in train_metrics:
                loss_str = (
                    f"Policy Loss: {train_metrics['avg_policy_loss']:.4f} | "
                    f"Value Loss: {train_metrics['avg_value_loss']:.4f} | "
                    f"LR: {train_metrics.get('avg_lr', 0):.2e}"
                )
            
            self.logger.info(
                f"Epoch {epoch+1}/{self.num_epochs} | "
                f"Reward: {train_metrics['avg_reward']:.4f} | "
                f"{loss_str} | "
                f"Time: {epoch_time:.1f}s"
            )
            
            # Validation
            if (epoch + 1) % self.save_freq == 0:
                val_metrics = self.evaluate(self.val_dataset, 'val')
                
                self.logger.info(
                    f"Validation | "
                    f"Recall@100: {val_metrics['recall@100']:.4f} | "
                    f"MRR: {val_metrics['mrr']:.4f}"
                )
                
                # Save checkpoint
                checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pt"
                save_checkpoint(
                    self.rl_trainer.agent_module,
                    self.rl_trainer.optimizer,
                    epoch,
                    val_metrics,
                    checkpoint_path
                )
                
                # Early stopping
                current_metric = val_metrics['mrr']
                
                if self.early_stopping(current_metric):
                    self.logger.info(f"Early stopping triggered at epoch {epoch+1}")
                    break
                
                if current_metric > best_metric:
                    best_metric = current_metric
                    best_path = self.checkpoint_dir / "best_model.pt"
                    save_checkpoint(
                        self.rl_trainer.agent_module,
                        self.rl_trainer.optimizer,
                        epoch,
                        val_metrics,
                        best_path
                    )
                    self.logger.info(f"Saved best model with MRR: {best_metric:.4f}")
            
            # Store metrics
            self.metrics_history['train_reward'].append(train_metrics['avg_reward'])
            self.metrics_history['epoch_time'].append(epoch_time)
        
        # ================================================================
        # FINAL CHECKPOINT: Always save at end of training
        # ================================================================
        self.logger.info("=" * 60)
        self.logger.info("Saving final checkpoint...")
        
        # Always run final validation if we have validation data
        if self.val_dataset and best_metric == 0.0:
            self.logger.info("Running final validation (skipped during training)...")
            val_metrics = self.evaluate(self.val_dataset, 'val')
            self.logger.info(
                f"Final Validation | "
                f"Recall@100: {val_metrics['recall@100']:.4f} | "
                f"MRR: {val_metrics['mrr']:.4f}"
            )
            best_metric = val_metrics['mrr']
        
        # Always save final model checkpoint
        final_checkpoint_path = self.checkpoint_dir / "final_model.pt"
        save_checkpoint(
            self.rl_trainer.agent_module,
            self.rl_trainer.optimizer,
            self.num_epochs - 1,
            {'final_reward': self.metrics_history['train_reward'][-1] if self.metrics_history['train_reward'] else 0},
            final_checkpoint_path
        )
        self.logger.info(f"‚úÖ Saved final model to: {final_checkpoint_path}")
        
        # Save best model if not already saved
        best_path = self.checkpoint_dir / "best_model.pt"
        if not best_path.exists():
            save_checkpoint(
                self.rl_trainer.agent_module,
                self.rl_trainer.optimizer,
                self.num_epochs - 1,
                {'mrr': best_metric, 'final_reward': self.metrics_history['train_reward'][-1] if self.metrics_history['train_reward'] else 0},
                best_path
            )
            self.logger.info(f"‚úÖ Saved best model to: {best_path}")
        
        # Final test evaluation
        if self.test_dataset:
            self.logger.info("Running final test evaluation...")
            
            if best_path.exists():
                load_checkpoint(self.rl_trainer.agent_module, best_path)
            
            test_metrics = self.evaluate(self.test_dataset, 'test')
            
            self.logger.info(
                f"Test Results | "
                f"Recall@100: {test_metrics['recall@100']:.4f} | "
                f"MRR: {test_metrics['mrr']:.4f}"
            )
            
            # Save test results
            with open(self.checkpoint_dir / "test_results.json", 'w') as f:
                json.dump(test_metrics, f, indent=2)
        
        self.logger.info("Training completed!")
        
        # Print final stats
        self.logger.info("\n" + "=" * 60)
        self.logger.info("Training Statistics:")
        self.logger.info(f"  Total epochs: {len(self.metrics_history['epoch_time'])}")
        self.logger.info(f"  Avg epoch time: {np.mean(self.metrics_history['epoch_time']):.1f}s")
        self.logger.info(f"  Final reward: {self.metrics_history['train_reward'][-1] if self.metrics_history['train_reward'] else 0:.4f}")
        self.logger.info(f"  Best MRR: {best_metric:.4f}")
        self.logger.info(f"  Embedding cache hit rate: {self.embedding_cache.stats()['hit_rate']:.2%}")
        self.logger.info("=" * 60)
        
        # ================================================================
        # HUGGINGFACE UPLOAD
        # ================================================================
        if self.hf_uploader and best_path.exists():
            self.logger.info("=" * 60)
            self.logger.info("üì§ Uploading to HuggingFace...")
            try:
                # Upload best model
                metrics_to_upload = {
                    'mrr': best_metric,
                    'final_reward': self.metrics_history['train_reward'][-1] if self.metrics_history['train_reward'] else 0,
                    'epochs_trained': len(self.metrics_history['epoch_time']),
                }
                
                # Add test metrics if available
                if self.test_dataset and 'test_metrics' in dir():
                    metrics_to_upload.update({
                        'test_recall@100': test_metrics.get('recall@100', 0),
                        'test_mrr': test_metrics.get('mrr', 0),
                    })
                
                self.hf_uploader.upload_checkpoint(
                    checkpoint_path=str(best_path),
                    metrics=metrics_to_upload,
                    config=self.config
                )
                self.logger.info(f"‚úÖ Uploaded best model to HuggingFace: {self.hf_repo_id}")
                
                # Also upload final model if different from best
                if final_checkpoint_path.exists() and final_checkpoint_path != best_path:
                    self.hf_uploader.upload_checkpoint(
                        checkpoint_path=str(final_checkpoint_path),
                        metrics={'final_reward': self.metrics_history['train_reward'][-1] if self.metrics_history['train_reward'] else 0},
                        config=self.config
                    )
                    self.logger.info(f"‚úÖ Uploaded final model to HuggingFace")
                    
            except Exception as e:
                self.logger.error(f"‚ùå HuggingFace upload failed: {e}")
            self.logger.info("=" * 60)


if __name__ == "__main__":
    print("Optimized RL Training Loop loaded successfully")
    print("Features:")
    print("  - Batched episode collection")
    print("  - Pre-computed embedding cache")
    print("  - Mixed precision training (FP16)")
    print("  - Multi-GPU support")
    print("  - Optimized replay buffer")