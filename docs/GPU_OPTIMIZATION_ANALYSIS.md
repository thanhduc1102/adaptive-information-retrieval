# ğŸ“Š PhÃ¢n TÃ­ch Chi Tiáº¿t Há»‡ Thá»‘ng RL Training & Tá»‘i Æ¯u GPU

## 1. PHÃ‚N TÃCH Váº¤N Äá»€: Táº I SAO GPU CHá»ˆ Sá»¬ Dá»¤NG 2%?

### 1.1 Luá»“ng Training Hiá»‡n Táº¡i (train_rl.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LUá»’NG TRAINING CÅ¨                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  for query in queries:           â† Xá»¬ LÃ TUáº¦N Tá»° Tá»ªNG QUERY    â”‚
â”‚      â”‚                                                           â”‚
â”‚      â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ Mine Candidates â”‚ â† BM25 search + TF-IDF (CPU-bound)         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚      â”‚                                                           â”‚
â”‚      â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ Embed Query    â”‚ â† TÃ­nh toÃ¡n embedding (cÃ³ thá»ƒ cache)        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚      â”‚                                                           â”‚
â”‚      â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ Embed Candidatesâ”‚ â† Láº¶P Láº I cho má»—i query (redundant!)       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚      â”‚                                                           â”‚
â”‚      â–¼                                                           â”‚
â”‚  for step in range(5):           â† Xá»¬ LÃ TUáº¦N Tá»° Tá»ªNG STEP     â”‚
â”‚      â”‚                                                           â”‚
â”‚      â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ RL Forward     â”‚ â† BATCH SIZE = 1 (khÃ´ng batching!)          â”‚
â”‚  â”‚ (select_action)â”‚   GPU chá»‰ xá»­ lÃ½ 1 sample táº¡i 1 thá»i Ä‘iá»ƒm    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚      â”‚                                                           â”‚
â”‚      â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ Search & Eval  â”‚ â† BM25 search láº¡i cho má»—i step (slow!)      â”‚
â”‚  â”‚ (reward)       â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚      â”‚                                                           â”‚
â”‚      â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ Store to Bufferâ”‚ â† CPU memory, transfer overhead             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚                                                                  â”‚
â”‚  if episode_count % 128 == 0:                                    â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚      â”‚ PPO Update     â”‚ â† Chá»‰ update sau 128 episodes           â”‚
â”‚      â”‚ (batch=32)     â”‚   GPU idle pháº§n lá»›n thá»i gian!          â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 CÃC BOTTLENECK CHÃNH

| Váº¥n Ä‘á» | MÃ´ táº£ | Impact |
|--------|-------|--------|
| **Sequential Processing** | Xá»­ lÃ½ tá»«ng query má»™t | GPU idle 90%+ |
| **No Batching in Collection** | `select_action` vá»›i batch_size=1 | GPU utilization < 5% |
| **Repeated Embeddings** | TÃ­nh embedding láº¡i cho má»—i query/term | Redundant computation |
| **CPU-GPU Transfer** | LiÃªn tá»¥c chuyá»ƒn data giá»¯a CPU vÃ  GPU | High latency |
| **Search Bottleneck** | BM25 search blocking trong má»—i step | CPU-bound operation |
| **Small PPO Batches** | batch_size=32 cho PPO update | Underutilize GPU memory |

### 1.3 VÃ Dá»¤ Cá»¤ THá»‚

Giáº£ sá»­ cÃ³ 1000 queries, má»—i episode cÃ³ 5 steps:

**Code cÅ©:**
```python
# Má»—i query xá»­ lÃ½ riÃªng láº»
for query_id in query_ids:  # 1000 láº§n
    trajectory, reward = self.collect_episode(query, qrels)  # Sequential
    
    # Trong collect_episode():
    for step in range(5):  # 5 steps
        # GPU forward vá»›i batch_size=1!
        action = self.rl_agent.select_action(
            query_emb.unsqueeze(0),  # [1, 512]
            current_emb.unsqueeze(0),  # [1, 512]
            candidate_embs.unsqueeze(0),  # [1, 50, 512]
            ...
        )
        # â†’ GPU nháº­n input ráº¥t nhá», pháº§n lá»›n cores idle
```

**Thá»i gian Æ°á»›c tÃ­nh:**
- 1000 queries Ã— 5 steps = 5000 forward passes
- Má»—i forward pass: ~10ms (chá»§ yáº¿u lÃ  overhead, khÃ´ng pháº£i computation)
- Tá»•ng: ~50 giÃ¢y chá»‰ cho forward passes
- GPU utilization: < 5%

---

## 2. GIáº¢I PHÃP Tá»I Æ¯U

### 2.1 KIáº¾N TRÃšC Má»šI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LUá»’NG TRAINING Má»šI (Optimized)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚           PHASE 1: PRE-COMPUTATION (1 láº§n)                  â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚  â€¢ Pre-compute ALL query embeddings                         â”‚â”‚
â”‚  â”‚  â€¢ Cache trong GPU memory                                    â”‚â”‚
â”‚  â”‚  â€¢ Hash-based lookup                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚           PHASE 2: PARALLEL PREPARATION                      â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚  ThreadPoolExecutor(workers=4):                              â”‚â”‚
â”‚  â”‚    â€¢ Mine candidates cho N queries Ä‘á»“ng thá»i                 â”‚â”‚
â”‚  â”‚    â€¢ Batch embed candidates                                  â”‚â”‚
â”‚  â”‚    â€¢ Chuáº©n bá»‹ EpisodeData objects                           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚           PHASE 3: BATCHED COLLECTION                        â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚  for batch in batches(episodes, size=32):                    â”‚â”‚
â”‚  â”‚      â”‚                                                       â”‚â”‚
â”‚  â”‚      â–¼                                                       â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚â”‚
â”‚  â”‚  â”‚ BATCHED RL FORWARD                         â”‚             â”‚â”‚
â”‚  â”‚  â”‚ â€¢ query_embs: [32, 512]                    â”‚             â”‚â”‚
â”‚  â”‚  â”‚ â€¢ candidate_embs: [32, 50, 512]            â”‚             â”‚â”‚
â”‚  â”‚  â”‚ â†’ GPU processes 32 samples simultaneously! â”‚             â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚â”‚
â”‚  â”‚      â”‚                                                       â”‚â”‚
â”‚  â”‚      â–¼                                                       â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚â”‚
â”‚  â”‚  â”‚ CACHED REWARD COMPUTATION                   â”‚             â”‚â”‚
â”‚  â”‚  â”‚ â€¢ Cache search results                      â”‚             â”‚â”‚
â”‚  â”‚  â”‚ â€¢ Avoid repeated searches                   â”‚             â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚           PHASE 4: OPTIMIZED PPO UPDATE                      â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚  â€¢ GPU-resident replay buffer                                â”‚â”‚
â”‚  â”‚  â€¢ Mixed precision (FP16)                                    â”‚â”‚
â”‚  â”‚  â€¢ Large batch size (64-128)                                 â”‚â”‚
â”‚  â”‚  â€¢ Mini-batch updates                                        â”‚â”‚
â”‚  â”‚  â€¢ Multi-GPU DataParallel                                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 CÃC Tá»I Æ¯U Cá»¤ THá»‚

#### A. EMBEDDING CACHE

```python
class EmbeddingCache:
    """
    Cache embeddings Ä‘á»ƒ trÃ¡nh tÃ­nh toÃ¡n láº¡i.
    
    TrÆ°á»›c: Má»—i query "machine learning" Ä‘Æ°á»£c embed 5 láº§n (má»—i step)
    Sau: Embed 1 láº§n, lookup tá»« cache
    
    Tiáº¿t kiá»‡m: ~80% computation cho embeddings
    """
    
    def __init__(self, max_size=200000):
        self.cache = {}  # hash -> embedding tensor
        
    def get(self, text: str) -> torch.Tensor:
        key = hash(text)
        if key in self.cache:
            return self.cache[key]  # O(1) lookup
        
        # Compute vÃ  cache
        embedding = self.embed_model.encode(text)
        self.cache[key] = embedding
        return embedding
    
    def get_batch(self, texts: List[str]) -> torch.Tensor:
        """Batch compute cho efficiency."""
        # Check cache first
        # Batch encode missing texts
        # Much faster than individual encodes!
```

#### B. GPU-RESIDENT REPLAY BUFFER

```python
class OptimizedReplayBuffer:
    """
    LÆ°u trá»±c tiáº¿p trÃªn GPU Ä‘á»ƒ trÃ¡nh transfer.
    
    TrÆ°á»›c: 
      - Store on CPU
      - Sample â†’ transfer to GPU
      - Overhead: ~5ms per batch
    
    Sau:
      - Pre-allocate trÃªn GPU
      - Sample trá»±c tiáº¿p
      - Overhead: ~0.1ms per batch
    """
    
    def __init__(self, capacity, device='cuda'):
        # Pre-allocate trÃªn GPU
        self.query_embs = torch.zeros(capacity, 512, device=device)
        self.candidate_embs = torch.zeros(capacity, 50, 512, device=device)
        # ...
        
    def sample(self, batch_size) -> Dict[str, torch.Tensor]:
        # KhÃ´ng cáº§n .to(device)!
        indices = torch.randint(0, self.size, (batch_size,), device=self.device)
        return {
            'query_emb': self.query_embs[indices],  # Already on GPU
            # ...
        }
```

#### C. BATCHED EPISODE COLLECTION

```python
def collect_batch_episodes(self, episode_data_list, batch_size=32):
    """
    Xá»­ lÃ½ nhiá»u episodes Ä‘á»“ng thá»i.
    
    TrÆ°á»›c: 1 query â†’ 1 forward pass â†’ batch_size=1
    Sau: 32 queries â†’ 1 forward pass â†’ batch_size=32
    
    GPU Utilization: 5% â†’ 60%+
    """
    
    # Stack all queries into batch
    batch_query_embs = torch.stack([d.query_emb for d in episode_data_list])
    # Shape: [32, 512]
    
    # Pad and stack candidates
    batch_candidate_embs = pad_and_stack(...)
    # Shape: [32, 50, 512]
    
    # Single forward pass for entire batch!
    actions, log_probs, values = self.rl_agent.select_action(
        batch_query_embs,  # [32, 512]
        batch_candidate_embs,  # [32, 50, 512]
        ...
    )
    # GPU processes all 32 simultaneously!
```

#### D. MIXED PRECISION TRAINING (FP16)

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Forward pass vá»›i FP16
with autocast():
    log_probs, values, entropy = agent.evaluate_actions(...)
    loss = policy_loss + value_loss + entropy_loss

# Backward vá»›i scaling
scaler.scale(loss).backward()
scaler.unscale_(optimizer)
clip_grad_norm_(agent.parameters(), max_grad_norm)
scaler.step(optimizer)
scaler.update()

# Benefits:
# - 2x faster matrix multiplication
# - 2x less memory usage
# - CÃ³ thá»ƒ tÄƒng batch size gáº¥p Ä‘Ã´i
```

#### E. MULTI-GPU (DataParallel)

```python
if torch.cuda.device_count() > 1:
    agent = nn.DataParallel(agent)
    # Tá»± Ä‘á»™ng split batch across GPUs
    # GPU 0: processes samples 0-15
    # GPU 1: processes samples 16-31
```

---

## 3. SO SÃNH HIá»†U NÄ‚NG

### 3.1 Theoretical Speedup

| Aspect | Before | After | Speedup |
|--------|--------|-------|---------|
| Episode Collection | Sequential | Batched (32x) | 10-20x |
| GPU Utilization | 2-5% | 60-80% | 15-30x |
| Embedding Computation | Repeated | Cached | 5x |
| CPU-GPU Transfer | Every sample | Pre-allocated | 50x |
| PPO Update | FP32, small batch | FP16, large batch | 2-3x |
| Memory Efficiency | Inefficient | Optimized | 2x |

### 3.2 Expected Results vá»›i 2x T4 GPUs

**Before (train_rl.py):**
- GPU Memory: 2% (300MB / 15GB)
- GPU Utilization: 2-5%
- Epoch time: ~30-60 minutes
- Total training (50 epochs): ~25-50 hours

**After (train_rl_optimized.py):**
- GPU Memory: 40-60% (6-9GB / 15GB)
- GPU Utilization: 60-80%
- Epoch time: ~3-5 minutes
- Total training (50 epochs): ~2.5-4 hours

**Speedup: 10-15x**

---

## 4. CÃCH Sá»¬ Dá»¤NG

### 4.1 Cháº¡y Training Tá»‘i Æ¯u

```bash
cd /kaggle/adaptive-information-retrieval/adaptive-ir-system

# Training vá»›i config tá»‘i Æ°u
python train_optimized.py \
    --config configs/msa_optimized_gpu.yaml \
    --device cuda \
    --epochs 50 \
    --test

# TÃ¹y chá»‰nh batch size (náº¿u OOM)
python train_optimized.py \
    --config configs/msa_optimized_gpu.yaml \
    --batch-size 32 \
    --epochs 50

# Disable mixed precision (debug)
python train_optimized.py \
    --config configs/msa_optimized_gpu.yaml \
    --no-amp
```

### 4.2 Monitor GPU Usage

```bash
# Terminal riÃªng
watch -n 1 nvidia-smi

# Hoáº·c dÃ¹ng nvitop (Ä‘áº¹p hÆ¡n)
pip install nvitop
nvitop
```

### 4.3 Config Quan Trá»ng

```yaml
training:
  # TÄƒng batch_size Ä‘á»ƒ sá»­ dá»¥ng GPU tá»‘t hÆ¡n
  batch_size: 64           # PPO update batch
  collect_batch_size: 32    # Episode collection batch
  
  # TÄƒng buffer cho better sampling
  buffer_size: 50000
  
  # Mixed precision
  use_amp: true
  
  # Episodes before update
  episodes_per_update: 256
```

---

## 5. TROUBLESHOOTING

### 5.1 Out of Memory (OOM)

```bash
# Giáº£m batch size
python train_optimized.py --batch-size 32

# Hoáº·c trong config:
training:
  batch_size: 32
  collect_batch_size: 16
```

### 5.2 GPU Váº«n Tháº¥p

Kiá»ƒm tra:
1. Data loading cÃ³ blocking khÃ´ng?
2. BM25 search cÃ³ quÃ¡ cháº­m khÃ´ng?
3. Embedding model cÃ³ trÃªn GPU khÃ´ng?

```python
# Debug
import torch
print(f"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"GPU Utilization: {torch.cuda.utilization()}%")
```

### 5.3 Convergence Issues

```yaml
# TÄƒng entropy coefficient
rl_agent:
  entropy_coef: 0.02  # Default: 0.01

# Giáº£m learning rate
training:
  learning_rate: 0.0001  # Default: 0.0003
```

---

## 6. Káº¾T LUáº¬N

CÃ¡c tá»‘i Æ°u chÃ­nh:

1. **Batched Processing**: Thay vÃ¬ xá»­ lÃ½ tá»«ng query, xá»­ lÃ½ 32 queries Ä‘á»“ng thá»i
2. **Embedding Cache**: Cache embeddings Ä‘á»ƒ trÃ¡nh tÃ­nh toÃ¡n láº¡i
3. **GPU-Resident Buffer**: LÆ°u data trÃªn GPU, trÃ¡nh transfer overhead
4. **Mixed Precision**: FP16 cho faster computation vÃ  less memory
5. **Multi-GPU**: DataParallel Ä‘á»ƒ sá»­ dá»¥ng cáº£ 2 T4 GPUs

Káº¿t quáº£ mong Ä‘á»£i:
- GPU utilization: 2% â†’ 60-80%
- Training speed: 10-15x faster
- CÃ³ thá»ƒ training 50 epochs trong 2-4 giá» thay vÃ¬ 25-50 giá»
