# HÆ¯á»šNG DáºªN TRAINING Há»† THá»NG ADAPTIVE IR

TÃ i liá»‡u nÃ y hÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c Ä‘á»ƒ training RL Agent cho há»‡ thá»‘ng Adaptive Information Retrieval.

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Chuáº©n bá»‹ MÃ´i trÆ°á»ng](#1-chuáº©n-bá»‹-mÃ´i-trÆ°á»ng)
2. [CÃ i Ä‘áº·t Dependencies](#2-cÃ i-Ä‘áº·t-dependencies)
3. [Táº£i vÃ  Chuáº©n bá»‹ Dá»¯ liá»‡u](#3-táº£i-vÃ -chuáº©n-bá»‹-dá»¯-liá»‡u)
4. [Cáº¥u hÃ¬nh Training](#4-cáº¥u-hÃ¬nh-training)
5. [Cháº¡y Training](#5-cháº¡y-training)
6. [GiÃ¡m sÃ¡t QuÃ¡ trÃ¬nh Training](#6-giÃ¡m-sÃ¡t-quÃ¡-trÃ¬nh-training)
7. [ÄÃ¡nh giÃ¡ Model](#7-Ä‘Ã¡nh-giÃ¡-model)
8. [Troubleshooting](#8-troubleshooting)

---

## 1. CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG

### 1.1. YÃªu cáº§u Há»‡ thá»‘ng

**Hardware tá»‘i thiá»ƒu**:
- CPU: 4 cores
- RAM: 16GB
- Disk: 50GB trá»‘ng
- GPU: NVIDIA GPU vá»›i 8GB+ VRAM (khuyáº¿n nghá»‹)

**Hardware khuyáº¿n nghá»‹**:
- CPU: 8+ cores
- RAM: 32GB+
- Disk: 100GB+ SSD
- GPU: NVIDIA GPU vá»›i 16GB+ VRAM (RTX 3090, A100)

**Software**:
- Python: 3.8, 3.9, hoáº·c 3.10
- CUDA: 11.7+ (náº¿u dÃ¹ng GPU)
- Java: 11+ (cho Pyserini)

### 1.2. Kiá»ƒm tra GPU (náº¿u cÃ³)

```bash
# Kiá»ƒm tra NVIDIA GPU
nvidia-smi

# Kiá»ƒm tra CUDA
nvcc --version

# Kiá»ƒm tra PyTorch cÃ³ nháº­n GPU khÃ´ng (sau khi cÃ i)
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
```

### 1.3. Táº¡o Virtual Environment

```bash
# Táº¡o environment
python3.9 -m venv venv

# KÃ­ch hoáº¡t
source venv/bin/activate  # Linux/Mac
# HOáº¶C
venv\Scripts\activate     # Windows

# NÃ¢ng cáº¥p pip
pip install --upgrade pip setuptools wheel
```

---

## 2. CÃ€I Äáº¶T DEPENDENCIES

### 2.1. Di chuyá»ƒn vÃ o thÆ° má»¥c dá»± Ã¡n

```bash
cd adaptive-information-retrieval/adaptive-ir-system
```

### 2.2. CÃ i Ä‘áº·t Java (cho Pyserini)

**Ubuntu/Debian**:
```bash
sudo apt update
sudo apt install -y openjdk-21-jdk

# Kiá»ƒm tra
java -version
```

**macOS**:
```bash
brew install openjdk@21

# Set JAVA_HOME
export JAVA_HOME=$(/usr/libexec/java_home -v21)
```

**Windows**:
- Download Java 21 tá»«: https://adoptium.net/
- CÃ i Ä‘áº·t vÃ  set JAVA_HOME trong Environment Variables

### 2.3. CÃ i Ä‘áº·t Python packages

```bash
# CÃ i Ä‘áº·t táº¥t cáº£ dependencies
pip install -r requirements.txt

# Hoáº·c cÃ i tá»«ng nhÃ³m:

# Core ML/DL
pip install torch>=2.0.0 transformers>=4.30.0 sentence-transformers>=2.2.0

# Information Retrieval
pip install pyserini>=0.21.0 rank-bm25>=0.2.2 pytrec-eval>=0.5

# Data Processing
pip install h5py>=3.8.0 pandas>=2.0.0 numpy>=1.24.0 nltk>=3.8 scikit-learn>=1.2.0

# Utilities
pip install pyyaml>=6.0 tqdm>=4.65.0 tensorboard>=2.13.0 wandb>=0.15.0

# Testing
pip install pytest>=7.3.0
```

### 2.4. Download NLTK data

```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### 2.5. XÃ¡c minh cÃ i Ä‘áº·t

```bash
# Kiá»ƒm tra cÃ¡c package chÃ­nh
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "from pyserini.search.lucene import LuceneSearcher; print('Pyserini: OK')"
```

---

## 3. Táº¢I VÃ€ CHUáº¨N Bá»Š Dá»® LIá»†U

CÃ³ 2 lá»±a chá»n dataset:
- **MS MARCO** (khuyáº¿n nghá»‹ - dataset chÃ­nh thá»©c, lá»›n)
- **Legacy datasets** (nhá» hÆ¡n, cho testing nhanh)

### OPTION A: MS MARCO (Khuyáº¿n nghá»‹)

#### 3.1. Táº¡o thÆ° má»¥c data

```bash
mkdir -p data/msmarco
cd data/msmarco
```

#### 3.2. Download MS MARCO dataset

**Tá»± Ä‘á»™ng** (khuyáº¿n nghá»‹):
```bash
cd ../../  # Quay vá» adaptive-ir-system
python scripts/download_msmarco.py \
  --data_dir ./data/msmarco \
  --subsets collection queries_train queries_dev qrels_train qrels_dev
```

**Thá»§ cÃ´ng**:
```bash
cd data/msmarco

# Collection (8.8M passages) - ~1GB
wget https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz
tar -xzf collection.tar.gz

# Training queries & qrels
wget https://msmarco.blob.core.windows.net/msmarcoranking/queries.train.tsv
wget https://msmarco.blob.core.windows.net/msmarcoranking/qrels.train.tsv

# Dev queries & qrels
wget https://msmarco.blob.core.windows.net/msmarcoranking/queries.dev.tsv
wget https://msmarco.blob.core.windows.net/msmarcoranking/qrels.dev.tsv

cd ../../
```

#### 3.3. Build BM25 Index

```bash
# Build index vá»›i Pyserini
python scripts/build_index.py \
  --collection ./data/msmarco/collection.tsv \
  --index ./data/msmarco/index \
  --threads 8

# QuÃ¡ trÃ¬nh nÃ y máº¥t 20-40 phÃºt
# YÃªu cáº§u ~10GB disk space cho index
```

**XÃ¡c minh index**:
```bash
# Test search
python -c "
from pyserini.search.lucene import LuceneSearcher
searcher = LuceneSearcher('./data/msmarco/index')
hits = searcher.search('what is covid', k=10)
print(f'Found {len(hits)} results')
print(f'Top result: {hits[0].raw}')
"
```

#### 3.4. Cáº¥u trÃºc thÆ° má»¥c sau khi setup

```
data/msmarco/
â”œâ”€â”€ collection.tsv           # 8.8M passages
â”œâ”€â”€ queries.train.tsv        # ~500K training queries
â”œâ”€â”€ queries.dev.tsv          # ~6,900 dev queries
â”œâ”€â”€ qrels.train.tsv          # Training relevance judgments
â”œâ”€â”€ qrels.dev.tsv            # Dev relevance judgments
â””â”€â”€ index/                   # BM25 index (~10GB)
    â”œâ”€â”€ segments_1
    â”œâ”€â”€ _0.cfs
    â””â”€â”€ ...
```

### OPTION B: Legacy Dataset (Nhanh hÆ¡n cho testing)

Náº¿u báº¡n cÃ³ legacy datasets (MSA, TREC-CAR, Jeopardy) trong thÆ° má»¥c `Query Reformulator/`:

```bash
# Kiá»ƒm tra dá»¯ liá»‡u
python scripts/test_legacy_data.py --data_dir "../Query Reformulator"

# Náº¿u cÃ³ msa_dataset.hdf5 vÃ  msa_corpus.hdf5
# Báº¡n cÃ³ thá»ƒ dÃ¹ng config msa_config.yaml Ä‘á»ƒ training
```

---

## 4. Cáº¤U HÃŒNH TRAINING

### 4.1. Táº¡o file config

Táº¡o file `configs/my_config.yaml`:

```yaml
# Cáº¥u hÃ¬nh há»‡ thá»‘ng
system:
  device: 'cuda'        # 'cuda' hoáº·c 'cpu'
  seed: 42
  num_workers: 4

# Cáº¥u hÃ¬nh dá»¯ liá»‡u
data:
  dataset_type: 'msmarco'
  data_dir: './data/msmarco'
  index_path: './data/msmarco/index'

  # File paths (tá»± Ä‘á»™ng tÃ¬m náº¿u khÃ´ng chá»‰ Ä‘á»‹nh)
  train_queries: './data/msmarco/queries.train.tsv'
  train_qrels: './data/msmarco/qrels.train.tsv'
  dev_queries: './data/msmarco/queries.dev.tsv'
  dev_qrels: './data/msmarco/qrels.dev.tsv'

# Embeddings (cho RL agent)
embeddings:
  type: 'sentence-transformers'
  model: 'all-MiniLM-L6-v2'  # 384-dim, nhanh

# Candidate Mining (Giai Ä‘oáº¡n 0)
candidate_mining:
  enabled: true
  max_candidates: 100        # Sá»‘ tá»« á»©ng viÃªn tá»‘i Ä‘a
  min_score: 0.1
  methods:
    - 'tfidf'
    - 'bm25'
  top_k_per_method: 50
  top_k_docs: 10             # Sá»‘ docs Ä‘á»ƒ mine candidates

# RL Agent (Giai Ä‘oáº¡n 1)
rl_agent:
  enabled: true
  embedding_dim: 384         # Match embedding model
  hidden_dim: 256
  num_attention_heads: 4
  num_encoder_layers: 2
  dropout: 0.1
  max_steps_per_episode: 5   # Tá»‘i Ä‘a 5 tá»« Ä‘Æ°á»£c thÃªm vÃ o
  num_query_variants: 4      # Táº¡o 4 query variants

  use_pretrained_embeddings: true
  embedding_model: 'all-MiniLM-L6-v2'

# RRF Fusion (Giai Ä‘oáº¡n 2)
rrf_fusion:
  enabled: true
  k_constant: 60
  method: 'rrf'

# BERT Re-ranker (Giai Ä‘oáº¡n 3)
bert_reranker:
  enabled: true
  model_name: 'cross-encoder/ms-marco-MiniLM-L-12-v2'
  batch_size: 128
  max_length: 512
  top_k_rerank: 100
  use_fp16: true             # FP16 nhanh hÆ¡n 2x

# Retrieval settings
retrieval:
  top_k: 100
  bm25_k1: 0.9
  bm25_b: 0.4

# Training hyperparameters
training:
  num_epochs: 50
  batch_size: 32
  learning_rate: 0.0003
  episodes_per_update: 128   # Update policy sau 128 episodes
  ppo_epochs: 4              # 4 PPO updates má»—i láº§n

  # PPO parameters
  gamma: 0.99                # Discount factor
  gae_lambda: 0.95           # GAE lambda
  clip_epsilon: 0.2          # PPO clip epsilon
  value_loss_coef: 0.5
  entropy_coef: 0.01         # Exploration bonus
  max_grad_norm: 0.5

  # Reward shaping
  reward_weights:
    recall: 0.7              # 70% weight cho Recall@100
    mrr: 0.3                 # 30% weight cho MRR@10

  # Checkpointing
  checkpoint_dir: './checkpoints'
  save_freq: 5               # Save má»—i 5 epochs

  # Early stopping
  early_stopping_patience: 10

  # Logging
  log_dir: './logs'

  # Replay buffer
  buffer_size: 10000

# Evaluation metrics
evaluation:
  metrics:
    - 'recall@10'
    - 'recall@50'
    - 'recall@100'
    - 'mrr@10'
    - 'ndcg@10'
    - 'map'
    - 'precision@10'
```

### 4.2. Config cho mÃ¡y yáº¿u (CPU hoáº·c GPU nhá»)

```yaml
# configs/low_resource_config.yaml
system:
  device: 'cpu'              # DÃ¹ng CPU

rl_agent:
  embedding_dim: 128         # Giáº£m dimension
  hidden_dim: 128
  num_attention_heads: 2
  num_encoder_layers: 1

bert_reranker:
  enabled: false             # Táº¯t BERT re-ranking Ä‘á»ƒ nhanh hÆ¡n

training:
  batch_size: 16             # Giáº£m batch size
  episodes_per_update: 64
```

---

## 5. CHáº Y TRAINING

### 5.1. Training cÆ¡ báº£n

```bash
# Training vá»›i config máº·c Ä‘á»‹nh
python train.py --config configs/my_config.yaml

# Chá»‰ Ä‘á»‹nh device
python train.py \
  --config configs/my_config.yaml \
  --device cuda

# Chá»‰ Ä‘á»‹nh sá»‘ epochs
python train.py \
  --config configs/my_config.yaml \
  --epochs 100

# Resume tá»« checkpoint
python train.py \
  --config configs/my_config.yaml \
  --checkpoint checkpoints/checkpoint_epoch_25.pt
```

### 5.2. Training vá»›i custom settings

```bash
# Training vá»›i custom seed
python train.py \
  --config configs/my_config.yaml \
  --seed 123 \
  --device cuda \
  --epochs 50
```

### 5.3. Cháº¡y training trong background (Linux/Mac)

```bash
# Cháº¡y trong background vÃ  log ra file
nohup python train.py \
  --config configs/my_config.yaml \
  --device cuda \
  > training.log 2>&1 &

# Xem log realtime
tail -f training.log

# Kiá»ƒm tra process
ps aux | grep train.py
```

### 5.4. Quy trÃ¬nh training chi tiáº¿t

Khi báº¡n cháº¡y `train.py`, há»‡ thá»‘ng thá»±c hiá»‡n:

```
1. Load config
2. Setup logging â†’ logs/train.log
3. Initialize search engine (BM25 index)
4. Load embedding model
5. Initialize pipeline:
   - Candidate Miner
   - RL Agent (Actor-Critic)
   - RRF Fusion
   - BERT Re-ranker

For each epoch (1 to num_epochs):

  For each query in training set:

    1. Mine candidates (Giai Ä‘oáº¡n 0)
       - BM25 search â†’ top-k docs
       - TF-IDF analysis
       - Extract 50-100 candidate terms

    2. Collect episode (Giai Ä‘oáº¡n 1):
       a. Evaluate original query â†’ metrics_before
       b. RL Agent selects terms iteratively:
          - Step 1: Select term_1 â†’ query' = query + term_1
          - Step 2: Select term_2 â†’ query'' = query' + term_2
          - ...
          - Step N: Select STOP
       c. Evaluate reformulated query â†’ metrics_after
       d. Compute reward = w1*Î”Recall + w2*Î”MRR
       e. Store (state, action, reward) to replay buffer

    3. Update policy every 128 episodes:
       - Sample batch from replay buffer
       - Compute advantages
       - PPO update (4 epochs)
       - Compute loss

  Validation every 5 epochs:
    - Evaluate on dev set
    - Compute metrics (Recall@100, MRR@10, nDCG@10)
    - Save checkpoint if best
    - Check early stopping

  Save checkpoint:
    - checkpoints/checkpoint_epoch_X.pt
    - checkpoints/best_model.pt (best validation)

Final test evaluation:
  - Load best model
  - Evaluate on test set
  - Save results â†’ checkpoints/test_results.json
```

---

## 6. GIÃM SÃT QUÃ TRÃŒNH TRAINING

### 6.1. Theo dÃµi qua logs

```bash
# Xem log realtime
tail -f logs/train.log

# TÃ¬m lá»—i
grep "ERROR" logs/train.log

# Xem validation metrics
grep "Validation" logs/train.log
```

**Log output máº«u**:
```
================================================================================
Adaptive IR System - Training
================================================================================
Random seed: 42
Device: cuda
Loading datasets...
Train queries: 502939
Val queries: 6980
Initializing search engine...
Index: ./data/msmarco/index
Building pipeline...
Loaded embedding model: all-MiniLM-L6-v2
Initialized Candidate Term Miner
Initialized RL Agent with 1,245,632 parameters
Initialized RRF Fusion (k=60)
Initialized BERT Re-ranker
Pipeline initialized successfully
Initializing training loop...
Starting training...
Epochs: 50
Batch size: 32
Episodes per update: 128
--------------------------------------------------------------------------------
Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502939/502939 [2:15:32<00:00, 61.8it/s, reward=0.0234, episodes=1024]
Epoch 1/50 | Reward: 0.0234 | Loss: 0.1234
Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502939/502939 [2:12:18<00:00, 63.2it/s, reward=0.0456, episodes=1024]
Epoch 5/50 | Reward: 0.0456 | Loss: 0.0987
Validation | Recall@100: 0.8123 | MRR@10: 0.3456
Saved best model with MRR@10: 0.3456
...
```

### 6.2. TensorBoard monitoring

```bash
# Khá»Ÿi Ä‘á»™ng TensorBoard (náº¿u enabled trong code)
tensorboard --logdir logs/ --port 6006

# Má»Ÿ browser: http://localhost:6006
```

### 6.3. Weights & Biases (W&B) monitoring

```bash
# Login W&B (one-time)
wandb login

# Training sáº½ tá»± Ä‘á»™ng log lÃªn W&B
# Xem táº¡i: https://wandb.ai/your-username/adaptive-ir
```

### 6.4. Kiá»ƒm tra GPU usage

```bash
# Monitor GPU realtime
watch -n 1 nvidia-smi

# Hoáº·c dÃ¹ng gpustat
pip install gpustat
gpustat -i 1
```

### 6.5. Æ¯á»›c tÃ­nh thá»i gian

**MS MARCO (~500K training queries)**:
- **GPU (RTX 3090)**: ~2-3 giá»/epoch â†’ 50 epochs = 100-150 giá» (4-6 ngÃ y)
- **GPU (V100)**: ~1.5-2 giá»/epoch â†’ 50 epochs = 75-100 giá» (3-4 ngÃ y)
- **CPU**: ~10-15 giá»/epoch â†’ KhÃ´ng khuyáº¿n nghá»‹

**Máº¹o tÄƒng tá»‘c**:
1. Giáº£m sá»‘ queries training (sample 10% â†’ nhanh hÆ¡n 10x)
2. Giáº£m `episodes_per_update` (128 â†’ 64)
3. Táº¯t BERT re-ranker trong training
4. DÃ¹ng FP16 mixed precision
5. TÄƒng batch size náº¿u GPU Ä‘á»§ RAM

---

## 7. ÄÃNH GIÃ MODEL

### 7.1. Kiá»ƒm tra checkpoints

```bash
# List checkpoints
ls -lh checkpoints/

# Output:
# checkpoint_epoch_5.pt    (120MB)
# checkpoint_epoch_10.pt   (120MB)
# checkpoint_epoch_15.pt   (120MB)
# best_model.pt            (120MB)
```

### 7.2. Load vÃ  test checkpoint

```python
# test_checkpoint.py
import torch
from src.pipeline import AdaptiveIRPipeline
from pyserini.search.lucene import LuceneSearcher
from sentence_transformers import SentenceTransformer

# Load config
import yaml
with open('configs/my_config.yaml') as f:
    config = yaml.safe_load(f)

# Setup components
searcher = LuceneSearcher('./data/msmarco/index')
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize pipeline
pipeline = AdaptiveIRPipeline(
    config=config,
    search_engine=searcher,
    embedding_model=embedding_model
)

# Load best model
pipeline.load_rl_checkpoint('checkpoints/best_model.pt')

# Test query
result = pipeline.search("what is covid-19", top_k=10, measure_latency=True)

print(f"Query: {result['query']}")
print(f"Query variants: {result['query_variants']}")
print(f"\nTop 10 results:")
for i, (doc_id, score) in enumerate(result['results'][:10], 1):
    doc = searcher.doc(doc_id)
    print(f"{i}. [{score:.4f}] {doc.raw()[:100]}...")

print(f"\nLatency:")
for stage, latency in result['latency'].items():
    print(f"  {stage}: {latency:.2f}ms")
```

```bash
python test_checkpoint.py
```

### 7.3. Evaluation script

```bash
# Evaluate trÃªn dev set
python scripts/final_test.py \
  --config configs/my_config.yaml \
  --checkpoint checkpoints/best_model.pt \
  --split dev

# Evaluate trÃªn test set
python scripts/final_test.py \
  --config configs/my_config.yaml \
  --checkpoint checkpoints/best_model.pt \
  --split test \
  --output results/test_results.json
```

### 7.4. So sÃ¡nh vá»›i baseline

```python
# compare_baseline.py
from src.evaluation import IRMetricsAggregator

# Baseline (BM25 only)
baseline_metrics = {
    'recall@100': 0.75,
    'mrr@10': 0.28,
    'ndcg@10': 0.32
}

# Your model
your_metrics = {
    'recall@100': 0.86,
    'mrr@10': 0.41,
    'ndcg@10': 0.45
}

print("Improvement:")
for metric in baseline_metrics:
    baseline = baseline_metrics[metric]
    yours = your_metrics[metric]
    improvement = (yours - baseline) / baseline * 100
    print(f"{metric}: {baseline:.4f} â†’ {yours:.4f} ({improvement:+.1f}%)")
```

---

## 8. TROUBLESHOOTING

### 8.1. Out of Memory (OOM)

**Triá»‡u chá»©ng**:
```
RuntimeError: CUDA out of memory. Tried to allocate 2.50 GiB
```

**Giáº£i phÃ¡p**:
```yaml
# Giáº£m batch_size trong config
training:
  batch_size: 16  # Tá»« 32 â†’ 16

# Giáº£m max_candidates
candidate_mining:
  max_candidates: 50  # Tá»« 100 â†’ 50

# Giáº£m embedding_dim
rl_agent:
  embedding_dim: 256  # Tá»« 512 â†’ 256
  hidden_dim: 128     # Tá»« 256 â†’ 128

# Hoáº·c dÃ¹ng gradient accumulation
training:
  batch_size: 8
  gradient_accumulation_steps: 4  # Effective batch = 32
```

### 8.2. Java/Pyserini errors

**Lá»—i**: `Module jdk.incubator.vector not found`

**Giáº£i phÃ¡p**:
```bash
# CÃ i Java 21
sudo apt install openjdk-21-jdk

# Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64
```

**Lá»—i**: `JVM cannot be started`

**Giáº£i phÃ¡p**:
```bash
# Restart Python interpreter
# Pyserini chá»‰ khá»Ÿi Ä‘á»™ng JVM má»™t láº§n, khÃ´ng thá»ƒ restart
# Pháº£i restart Python process
```

### 8.3. Training khÃ´ng converge

**Triá»‡u chá»©ng**: Reward khÃ´ng tÄƒng sau nhiá»u epochs

**Giáº£i phÃ¡p**:
```yaml
# 1. Giáº£m learning rate
training:
  learning_rate: 0.0001  # Tá»« 0.0003

# 2. TÄƒng exploration
training:
  entropy_coef: 0.02  # Tá»« 0.01

# 3. Thay Ä‘á»•i reward weights
training:
  reward_weights:
    recall: 0.8  # TÄƒng recall weight
    mrr: 0.2

# 4. TÄƒng episodes_per_update
training:
  episodes_per_update: 256  # Tá»« 128
```

### 8.4. Slow training

**Giáº£i phÃ¡p**:
```yaml
# 1. Táº¯t BERT re-ranker trong training
bert_reranker:
  enabled: false

# 2. Giáº£m sá»‘ query variants
rl_agent:
  num_query_variants: 2  # Tá»« 4

# 3. Giáº£m sá»‘ docs Ä‘á»ƒ mine
candidate_mining:
  top_k_docs: 5  # Tá»« 10

# 4. Sample subset cá»§a training data
# Trong train.py, thÃªm:
# query_ids = query_ids[:50000]  # Chá»‰ láº¥y 50K queries
```

### 8.5. Index not found

**Lá»—i**: `FileNotFoundError: Index not found at ./data/msmarco/index`

**Giáº£i phÃ¡p**:
```bash
# Build index
python scripts/build_index.py \
  --collection ./data/msmarco/collection.tsv \
  --index ./data/msmarco/index

# Hoáº·c update path trong config
data:
  index_path: '/absolute/path/to/index'
```

### 8.6. Queries file not found

**Giáº£i phÃ¡p**:
```bash
# Download láº¡i
python scripts/download_msmarco.py \
  --data_dir ./data/msmarco \
  --subsets queries_train queries_dev qrels_train qrels_dev
```

### 8.7. Reward luÃ´n = 0

**NguyÃªn nhÃ¢n**: KhÃ´ng cÃ³ qrels cho queries

**Giáº£i phÃ¡p**:
```python
# Kiá»ƒm tra qrels
import pandas as pd
qrels = pd.read_csv('data/msmarco/qrels.train.tsv', sep='\t', header=None)
print(f"Number of qrels: {len(qrels)}")
print(qrels.head())

# Äáº£m báº£o query_id trong queries cÃ³ trong qrels
```

---

## 9. TIPS & BEST PRACTICES

### 9.1. Development workflow

```bash
# 1. Test trÃªn subset nhá» trÆ°á»›c
# Sá»­a train.py Ä‘á»ƒ chá»‰ láº¥y 1000 queries:
query_ids = query_ids[:1000]

# 2. Training nhanh (5 epochs) Ä‘á»ƒ verify code
python train.py --config configs/my_config.yaml --epochs 5

# 3. Náº¿u OK, cháº¡y full training
python train.py --config configs/my_config.yaml --epochs 50
```

### 9.2. Experiment tracking

```bash
# Táº¡o folder riÃªng cho má»—i experiment
mkdir -p experiments/exp_001_baseline
mkdir -p experiments/exp_002_higher_lr

# Copy config
cp configs/my_config.yaml experiments/exp_001_baseline/config.yaml

# Training
python train.py --config experiments/exp_001_baseline/config.yaml

# Log results
echo "Exp 001: Recall@100=0.86, MRR@10=0.41" >> experiments/results.txt
```

### 9.3. Hyperparameter tuning

**Thá»© tá»± Æ°u tiÃªn**:
1. `learning_rate`: [0.0001, 0.0003, 0.001]
2. `reward_weights`: Thá»­ nhiá»u tá»· lá»‡ recall/mrr
3. `num_query_variants`: [2, 3, 4, 5]
4. `max_candidates`: [50, 100, 150]
5. `hidden_dim`: [128, 256, 512]

### 9.4. Debugging

```python
# Enable verbose logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Print intermediate results
# Trong collect_episode(), thÃªm:
print(f"Original query: {query}")
print(f"Candidates: {list(candidates.keys())[:10]}")
print(f"Selected terms: {selected_terms}")
print(f"Reformulated query: {current_query}")
print(f"Reward: {reward}")
```

---

## 10. CHECKLIST TRÆ¯á»šC KHI TRAINING

- [ ] Java 11+ Ä‘Ã£ cÃ i vÃ  JAVA_HOME Ä‘Ã£ set
- [ ] Python 3.8+ Ä‘Ã£ cÃ i
- [ ] Virtual environment Ä‘Ã£ táº¡o vÃ  activate
- [ ] Dependencies Ä‘Ã£ cÃ i (`pip install -r requirements.txt`)
- [ ] NLTK data Ä‘Ã£ download
- [ ] Dataset Ä‘Ã£ download (MS MARCO hoáº·c legacy)
- [ ] BM25 index Ä‘Ã£ build
- [ ] Config file Ä‘Ã£ táº¡o vÃ  review
- [ ] Disk space Ä‘á»§ (Ã­t nháº¥t 50GB)
- [ ] GPU driver vÃ  CUDA Ä‘Ã£ cÃ i (náº¿u dÃ¹ng GPU)
- [ ] Test search vá»›i index thÃ nh cÃ´ng

---

## 11. Lá»†NH TRAINING HOÃ€N CHá»ˆNH

```bash
# 1. Setup mÃ´i trÆ°á»ng
cd adaptive-information-retrieval/adaptive-ir-system
source venv/bin/activate

# 2. Download data (náº¿u chÆ°a cÃ³)
python scripts/download_msmarco.py --data_dir ./data/msmarco

# 3. Build index (náº¿u chÆ°a cÃ³)
python scripts/build_index.py \
  --collection ./data/msmarco/collection.tsv \
  --index ./data/msmarco/index

# 4. Verify setup
python -c "from pyserini.search.lucene import LuceneSearcher; \
  searcher = LuceneSearcher('./data/msmarco/index'); \
  print(f'Index OK: {searcher.num_docs} docs')"

# 5. Training
python train.py \
  --config configs/my_config.yaml \
  --device cuda \
  --epochs 50 \
  2>&1 | tee training.log

# 6. Monitor (terminal khÃ¡c)
tail -f logs/train.log
watch -n 5 nvidia-smi
```

---

## 12. Káº¾T QUáº¢ MáºªU

Sau khi training xong, báº¡n sáº½ cÃ³:

```
checkpoints/
â”œâ”€â”€ checkpoint_epoch_5.pt
â”œâ”€â”€ checkpoint_epoch_10.pt
â”œâ”€â”€ checkpoint_epoch_15.pt
â”œâ”€â”€ ...
â”œâ”€â”€ checkpoint_epoch_50.pt
â”œâ”€â”€ best_model.pt              # Model tá»‘t nháº¥t
â””â”€â”€ test_results.json          # Káº¿t quáº£ test

logs/
â””â”€â”€ train.log                  # Training logs

tensorboard/                   # TensorBoard logs (náº¿u enabled)
```

**test_results.json**:
```json
{
  "recall@10": 0.4523,
  "recall@50": 0.7234,
  "recall@100": 0.8612,
  "mrr@10": 0.4123,
  "ndcg@10": 0.4567,
  "map": 0.3987,
  "precision@10": 0.3456
}
```

---

**ChÃºc báº¡n training thÃ nh cÃ´ng!** ğŸš€

Náº¿u gáº·p váº¥n Ä‘á», hÃ£y:
1. Kiá»ƒm tra logs: `logs/train.log`
2. Xem láº¡i pháº§n Troubleshooting
3. Giáº£m config xuá»‘ng Ä‘á»ƒ test trÃªn subset nhá»
4. Má»Ÿ issue trÃªn GitHub repo
