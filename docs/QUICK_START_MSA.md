# QUICK START: TRAINING Vá»šI MSA DATASET

Data Ä‘Ã£ sáºµn sÃ ng! HÆ°á»›ng dáº«n training nhanh.

---

## âœ… Checklist Data

- âœ… `msa_dataset.hdf5`: 452M - 271,345 training queries
- âœ… `msa_corpus.hdf5`: 459M - 480,722 documents
- âœ… `D_cbow_pdw_8B.pkl`: 732M - 374,557 words embeddings (500-dim)

---

## ğŸš€ BÆ¯á»šC 1: CÃ€I Äáº¶T DEPENDENCIES

```bash
cd /Users/vanhkhongpeo/Documents/Github/Adaptive_information_retrival/adaptive-information-retrieval/adaptive-ir-system

# Táº¡o virtual environment (náº¿u chÆ°a cÃ³)
python3 -m venv venv
source venv/bin/activate

# CÃ i packages
pip install --upgrade pip
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

---

## ğŸš€ BÆ¯á»šC 2: KIá»‚M TRA CONFIG

File config Ä‘Ã£ cÃ³ sáºµn táº¡i `configs/msa_config.yaml`:

```yaml
# configs/msa_config.yaml
system:
  device: 'cuda'      # Äá»•i thÃ nh 'cpu' náº¿u khÃ´ng cÃ³ GPU
  seed: 42

data:
  dataset_type: 'msa'
  data_dir: '../Query Reformulator'

embeddings:
  type: 'legacy'
  path: '../Query Reformulator/D_cbow_pdw_8B.pkl'
  embedding_dim: 500

rl_agent:
  embedding_dim: 500
  hidden_dim: 256
  max_steps_per_episode: 5
  num_query_variants: 4

training:
  num_epochs: 50
  batch_size: 32
  learning_rate: 0.0003
  episodes_per_update: 128
```

**Náº¿u mÃ¡y yáº¿u**, sá»­a config:
```yaml
system:
  device: 'cpu'       # DÃ¹ng CPU

rl_agent:
  hidden_dim: 128     # Giáº£m tá»« 256 â†’ 128

training:
  batch_size: 16      # Giáº£m tá»« 32 â†’ 16
  num_epochs: 20      # Test vá»›i 20 epochs trÆ°á»›c
```

---

## ğŸš€ BÆ¯á»šC 3: TEST DATA LOADER

TrÆ°á»›c khi training, test xem data load Ä‘Ãºng khÃ´ng:

```bash
cd adaptive-ir-system

# Test load data
python scripts/test_legacy_data.py --data_dir "../Query Reformulator"
```

**Expected output**:
```
Testing legacy dataset: msa
âœ“ Dataset loaded successfully
  Queries: 271,345
  Documents: 480,722
  Embeddings: 374,557 words

Sample query: "the metabolic code"
Sample doc: "hybrid compactifications and brane gravity..."
âœ“ All data valid!
```

---

## ğŸš€ BÆ¯á»šC 4: TRAINING THáº¬T

### Option A: Training Ä‘áº§y Ä‘á»§ (GPU khuyáº¿n nghá»‹)

```bash
cd adaptive-ir-system

# Training vá»›i GPU
python train.py \
  --config configs/msa_config.yaml \
  --device cuda \
  --epochs 50

# Log sáº½ á»Ÿ: logs_msa/train.log
# Checkpoints sáº½ á»Ÿ: checkpoints_msa/
```

**Thá»i gian Æ°á»›c tÃ­nh**:
- **GPU (RTX 3090)**: ~30-45 phÃºt/epoch â†’ 50 epochs = **25-37 giá»**
- **GPU (V100)**: ~20-30 phÃºt/epoch â†’ 50 epochs = **17-25 giá»**
- **CPU**: ~2-4 giá»/epoch â†’ KhÃ´ng khuyáº¿n nghá»‹

### Option B: Test training vá»›i subset nhá» (CPU OK)

```bash
# Test vá»›i 1000 queries, 5 epochs
python train.py \
  --config configs/msa_config.yaml \
  --device cpu \
  --epochs 5
```

Trong file `train.py`, thÃªm dÃ²ng nÃ y sau dÃ²ng 343:
```python
query_ids = query_ids[:1000]  # Chá»‰ láº¥y 1000 queries
```

---

## ğŸš€ BÆ¯á»šC 5: GIÃM SÃT TRAINING

### Terminal 1: Cháº¡y training
```bash
python train.py --config configs/msa_config.yaml
```

### Terminal 2: Theo dÃµi logs
```bash
# Xem log realtime
tail -f logs_msa/train.log

# Hoáº·c dÃ¹ng grep Ä‘á»ƒ lá»c
tail -f logs_msa/train.log | grep "Epoch"
tail -f logs_msa/train.log | grep "Validation"
```

### Terminal 3: Monitor GPU (náº¿u dÃ¹ng GPU)
```bash
watch -n 1 nvidia-smi
```

---

## ğŸ“Š BÆ¯á»šC 6: ÄÃNH GIÃ Káº¾T QUáº¢

Sau khi training xong, kiá»ƒm tra checkpoints:

```bash
# List checkpoints
ls -lh checkpoints_msa/

# Output:
# checkpoint_epoch_5.pt
# checkpoint_epoch_10.pt
# ...
# best_model.pt  â† Model tá»‘t nháº¥t
```

Test model:

```bash
# Test vá»›i checkpoint tá»‘t nháº¥t
python inference.py \
  --config configs/msa_config.yaml \
  --checkpoint checkpoints_msa/best_model.pt \
  --query "machine learning deep neural networks"
```

---

## ğŸ“ˆ Káº¾T QUáº¢ MONG Äá»¢I

### Baseline (BM25 only):
- Recall@10: ~0.30
- Recall@40: ~0.45
- MRR@10: ~0.20

### Sau training RL:
- Recall@10: ~0.35-0.40 (+17-33%)
- Recall@40: ~0.50-0.55 (+11-22%)
- MRR@10: ~0.25-0.30 (+25-50%)

---

## ğŸ”§ TROUBLESHOOTING

### Lá»—i: "No module named 'rank_bm25'"
```bash
pip install rank-bm25
```

### Lá»—i: Out of Memory
```yaml
# Sá»­a config:
training:
  batch_size: 8       # Giáº£m tá»« 32
rl_agent:
  hidden_dim: 128     # Giáº£m tá»« 256
```

### Lá»—i: Training quÃ¡ cháº­m
```yaml
# Táº¯t BERT re-ranker trong training
bert_reranker:
  enabled: false

# Giáº£m candidates
candidate_mining:
  max_candidates: 50  # Tá»« 100
```

### Lá»—i: Can't load embeddings
```python
# Test embeddings
import pickle
with open('../Query Reformulator/D_cbow_pdw_8B.pkl', 'rb') as f:
    emb = pickle.load(f, encoding='latin1')
    print(f"Loaded {len(emb)} words")
```

---

## ğŸ“ LOG OUTPUT MáºªU

```
================================================================================
Adaptive IR System - Training
================================================================================
Random seed: 42
Device: cuda
Loading datasets...
Train queries: 271345
Val queries: 20000
Initializing search engine...
Search engine: SimpleBM25 (legacy dataset)
Building pipeline...
Loaded legacy Word2Vec embeddings from ../Query Reformulator/D_cbow_pdw_8B.pkl
Initialized Candidate Term Miner
Initialized RL Agent with 1,245,632 parameters
Initialized RRF Fusion (k=60)
Initialized BERT Re-ranker
Pipeline initialized successfully
Starting training...
Epochs: 50
Batch size: 32
Episodes per update: 128
--------------------------------------------------------------------------------
Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 271345/271345 [38:24<00:00, 117.8it/s, reward=0.0123, episodes=2048]
Epoch 1/50 | Reward: 0.0123 | Loss: 0.2341
Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 271345/271345 [36:12<00:00, 124.9it/s, reward=0.0567, episodes=2048]
Epoch 5/50 | Reward: 0.0567 | Loss: 0.1234
Validation | Recall@10: 0.3234 | Recall@40: 0.4567 | MRR@10: 0.2123
Saved best model with MRR@10: 0.2123
...
```

---

## ğŸ¯ Lá»†NH HOÃ€N CHá»ˆNH

```bash
# Setup
cd /Users/vanhkhongpeo/Documents/Github/Adaptive_information_retrival/adaptive-information-retrieval/adaptive-ir-system
source venv/bin/activate  # Náº¿u dÃ¹ng venv

# Install (one-time)
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"

# Test data
python scripts/test_legacy_data.py --data_dir "../Query Reformulator"

# Training
python train.py \
  --config configs/msa_config.yaml \
  --device cuda \
  --epochs 50 \
  2>&1 | tee training.log

# Monitor (terminal khÃ¡c)
tail -f logs_msa/train.log
watch -n 1 nvidia-smi

# Evaluate
python inference.py \
  --config configs/msa_config.yaml \
  --checkpoint checkpoints_msa/best_model.pt \
  --query "deep learning neural networks"
```

---

## ğŸ’¡ TIPS

1. **Test trÆ°á»›c vá»›i subset nhá»**: Sá»­a `query_ids = query_ids[:1000]` trong `train.py`
2. **DÃ¹ng CPU cho test nhanh**: `--device cpu --epochs 5`
3. **Save checkpoints thÆ°á»ng xuyÃªn**: Config Ä‘Ã£ set `save_freq: 5`
4. **Monitor GPU**: Äáº£m báº£o GPU utilization > 80%
5. **Early stopping**: Sáº½ tá»± Ä‘á»™ng dá»«ng náº¿u khÃ´ng cáº£i thiá»‡n sau 10 epochs

---

**ChÃºc báº¡n training thÃ nh cÃ´ng!** ğŸš€

Náº¿u gáº·p váº¥n Ä‘á», check:
1. `logs_msa/train.log`
2. Pháº§n Troubleshooting á»Ÿ trÃªn
3. Hoáº·c há»i tÃ´i!
