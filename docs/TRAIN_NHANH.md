# HÆ¯á»šNG DáºªN TRAIN NHANH - TEST Vá»šI SUBSET NHá»

TÃ i liá»‡u nÃ y hÆ°á»›ng dáº«n train nhanh vá»›i subset nhá» Ä‘á»ƒ test há»‡ thá»‘ng (10-30 phÃºt thay vÃ¬ hÃ ng chá»¥c giá»).

---

## ğŸ¯ Má»¥c tiÃªu

- âœ… Verify code hoáº¡t Ä‘á»™ng Ä‘Ãºng
- âœ… Xem káº¿t quáº£ ban Ä‘áº§u nhÆ° tháº¿ nÃ o
- âœ… Test trÃªn laptop/mÃ¡y yáº¿u
- âœ… Debug nhanh
- â±ï¸ Thá»i gian: **10-30 phÃºt** (thay vÃ¬ 25-37 giá»)

---

## ğŸš€ OPTION 1: TRAIN Cá»°C NHANH (10-15 phÃºt)

### CÃ i Ä‘áº·t dependencies (náº¿u chÆ°a cÃ³)

```bash
cd /Users/vanhkhongpeo/Documents/Github/Adaptive_information_retrival/adaptive-information-retrieval/adaptive-ir-system

# CÃ i packages cÆ¡ báº£n
pip install torch numpy scikit-learn h5py rank-bm25 nltk tqdm pyyaml

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### Train vá»›i 500 queries, 5 epochs

```bash
# Training vá»›i 500 queries, 5 epochs (~10-15 phÃºt)
python train_quick_test.py \
  --config configs/msa_quick_test.yaml \
  --num_samples 500 \
  --epochs 5 \
  --device cuda

# Hoáº·c CPU (cháº­m hÆ¡n ~3x)
python train_quick_test.py \
  --config configs/msa_quick_test.yaml \
  --num_samples 500 \
  --epochs 5 \
  --device cpu
```

**Thá»i gian Æ°á»›c tÃ­nh**:
- GPU: ~10-15 phÃºt
- CPU: ~30-45 phÃºt

---

## ğŸš€ OPTION 2: TRAIN Vá»ªA PHáº¢I (20-30 phÃºt)

### Train vá»›i 1000 queries, 10 epochs

```bash
# Training vá»›i 1000 queries, 10 epochs (~20-30 phÃºt)
python train_quick_test.py \
  --config configs/msa_quick_test.yaml \
  --num_samples 1000 \
  --epochs 10 \
  --device cuda
```

**Thá»i gian Æ°á»›c tÃ­nh**:
- GPU: ~20-30 phÃºt
- CPU: ~60-90 phÃºt

---

## ğŸš€ OPTION 3: TRAIN Äáº¦Y Äá»¦ HÆ N (1-2 giá»)

### Train vá»›i 5000 queries, 10 epochs

```bash
# Training vá»›i 5000 queries, 10 epochs (~1-2 giá»)
python train_quick_test.py \
  --config configs/msa_quick_test.yaml \
  --num_samples 5000 \
  --epochs 10 \
  --device cuda
```

**Thá»i gian Æ°á»›c tÃ­nh**:
- GPU: ~1-2 giá»
- CPU: ~4-6 giá»

---

## ğŸ“Š SO SÃNH CÃC OPTION

| Option | Queries | Epochs | Thá»i gian (GPU) | Thá»i gian (CPU) | Khi nÃ o dÃ¹ng |
|--------|---------|--------|-----------------|-----------------|--------------|
| **1. Cá»±c nhanh** | 500 | 5 | 10-15 min | 30-45 min | Test code, debug |
| **2. Vá»«a pháº£i** | 1,000 | 10 | 20-30 min | 60-90 min | Xem káº¿t quáº£ sÆ¡ bá»™ |
| **3. Äáº§y Ä‘á»§ hÆ¡n** | 5,000 | 10 | 1-2 giá» | 4-6 giá» | Káº¿t quáº£ Ä‘Ã¡ng tin hÆ¡n |
| **Full** | 271,345 | 50 | 25-37 giá» | Nhiá»u ngÃ y | Production |

---

## ğŸ”§ CÃC THAY Äá»”I Äá»‚ TRAIN NHANH

Config `msa_quick_test.yaml` Ä‘Ã£ tá»‘i Æ°u Ä‘á»ƒ train nhanh:

### 1. Giáº£m kÃ­ch thÆ°á»›c model
```yaml
rl_agent:
  hidden_dim: 128              # Tá»« 256 â†’ 128
  num_attention_heads: 2       # Tá»« 4 â†’ 2
  num_encoder_layers: 1        # Tá»« 2 â†’ 1
```

### 2. Giáº£m sá»‘ query variants
```yaml
rl_agent:
  max_steps_per_episode: 3     # Tá»« 5 â†’ 3
  num_query_variants: 2        # Tá»« 4 â†’ 2
```

### 3. Giáº£m sá»‘ candidates
```yaml
candidate_mining:
  max_candidates: 30           # Tá»« 50 â†’ 30
  top_k_docs: 5                # Tá»« 10 â†’ 5
```

### 4. Táº¯t BERT re-ranking
```yaml
bert_reranker:
  enabled: false               # Táº¯t hoÃ n toÃ n
```

### 5. Giáº£m tham sá»‘ training
```yaml
training:
  batch_size: 16               # Tá»« 32 â†’ 16
  episodes_per_update: 64      # Tá»« 128 â†’ 64
  ppo_epochs: 2                # Tá»« 4 â†’ 2
```

---

## ğŸ“ˆ Káº¾T QUáº¢ MONG Äá»¢I

### Vá»›i 500 queries, 5 epochs:
- **Má»¥c Ä‘Ã­ch**: Verify code hoáº¡t Ä‘á»™ng
- **Káº¿t quáº£**: CÃ³ thá»ƒ chÆ°a tá»‘t, reward cÃ³ thá»ƒ Ã¢m hoáº·c gáº§n 0
- **Cháº¥p nháº­n Ä‘Æ°á»£c**: Chá»‰ cáº§n khÃ´ng lá»—i

### Vá»›i 1000 queries, 10 epochs:
- **Recall@10**: 0.25-0.30 (baseline: ~0.28)
- **Recall@40**: 0.40-0.45 (baseline: ~0.42)
- **MRR@10**: 0.18-0.22 (baseline: ~0.19)
- **Káº¿t quáº£**: CÃ³ thá»ƒ ngang hoáº·c hÆ¡i tá»‘t hÆ¡n baseline

### Vá»›i 5000 queries, 10 epochs:
- **Recall@10**: 0.30-0.35 (baseline: ~0.28)
- **Recall@40**: 0.45-0.50 (baseline: ~0.42)
- **MRR@10**: 0.22-0.26 (baseline: ~0.19)
- **Káº¿t quáº£**: ÄÃ¡ng tin cáº­y hÆ¡n, tháº¥y Ä‘Æ°á»£c cáº£i thiá»‡n

---

## ğŸ“ LOG OUTPUT MáºªU

```
================================================================================
Adaptive IR System - QUICK TEST Training
================================================================================
âš ï¸  SUBSET MODE: Training with 1,000 queries only
Random seed: 42
Device: cuda
Loading datasets...
Train queries: 1000
Val queries: 20000
Initializing search engine...
Search engine: SimpleBM25 (legacy dataset)
Building pipeline...
Loaded legacy Word2Vec embeddings from ../Query Reformulator/D_cbow_pdw_8B.pkl
  Embeddings: 374,557 words
Initialized Candidate Term Miner
Initialized RL Agent with 523,264 parameters (reduced model)
Initialized RRF Fusion (k=60)
BERT Re-ranker: DISABLED (for speed)
Pipeline initialized successfully
Starting training...
Epochs: 10
Batch size: 16
Episodes per update: 64
âš ï¸  Training queries: 1,000 (subset)
--------------------------------------------------------------------------------
Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:32<00:00, 10.8it/s, reward=-0.0023, episodes=64]
Epoch 1/10 | Reward: -0.0023 | Loss: 0.3456
Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:28<00:00, 11.3it/s, reward=0.0012, episodes=64]
Epoch 2/10 | Reward: 0.0012 | Loss: 0.2234
Validation | Recall@10: 0.2845 | Recall@40: 0.4312 | MRR@10: 0.1923
Saved checkpoint to checkpoints_msa_quick/checkpoint_epoch_2.pt
...
Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:25<00:00, 11.7it/s, reward=0.0234, episodes=64]
Epoch 10/10 | Reward: 0.0234 | Loss: 0.1123
Validation | Recall@10: 0.3123 | Recall@40: 0.4678 | MRR@10: 0.2245
Saved best model with MRR@10: 0.2245
================================================================================
Training completed!
================================================================================
Total time: 15 minutes 23 seconds
```

---

## ğŸ” KIá»‚M TRA Káº¾T QUáº¢

### 1. Xem checkpoints

```bash
ls -lh checkpoints_msa_quick/

# Output:
# checkpoint_epoch_2.pt
# checkpoint_epoch_4.pt
# checkpoint_epoch_6.pt
# checkpoint_epoch_8.pt
# checkpoint_epoch_10.pt
# best_model.pt
```

### 2. Test model

```bash
# Test inference
python << 'EOF'
import sys
sys.path.insert(0, 'src')

from src.pipeline import AdaptiveIRPipeline
from src.utils.legacy_embeddings import LegacyEmbeddingAdapter
from src.utils.simple_searcher import SimpleBM25Searcher
from src.utils.data_loader import DatasetFactory
import yaml

# Load config
with open('configs/msa_quick_test.yaml') as f:
    config = yaml.safe_load(f)

# Load data
dataset_factory = DatasetFactory(config['data'])
dataset = dataset_factory.create_dataset('train')

# Setup searcher
searcher = SimpleBM25Searcher(dataset)

# Load embeddings
embeddings = LegacyEmbeddingAdapter('../Query Reformulator/D_cbow_pdw_8B.pkl')

# Initialize pipeline
pipeline = AdaptiveIRPipeline(
    config=config,
    search_engine=searcher,
    embedding_model=embeddings
)

# Load best model
pipeline.load_rl_checkpoint('checkpoints_msa_quick/best_model.pt')

# Test query
query = "deep learning neural networks"
result = pipeline.search(query, top_k=10, measure_latency=True)

print(f"Query: {result['query']}")
print(f"\nQuery variants:")
for i, variant in enumerate(result['query_variants'], 1):
    print(f"  {i}. {variant}")

print(f"\nTop 5 results:")
for i, (doc_id, score) in enumerate(result['results'][:5], 1):
    print(f"  {i}. [Score: {score:.4f}] Doc ID: {doc_id}")

print(f"\nLatency:")
for stage, time_ms in result['latency'].items():
    print(f"  {stage}: {time_ms:.1f}ms")
EOF
```

### 3. So sÃ¡nh vá»›i baseline

```bash
# ÄÃ¡nh giÃ¡ trÃªn validation set
python << 'EOF'
from src.evaluation import IRMetricsAggregator

# Káº¿t quáº£ quick test
quick_metrics = {
    'recall@10': 0.3123,
    'recall@40': 0.4678,
    'mrr@10': 0.2245
}

# Baseline (BM25 only)
baseline_metrics = {
    'recall@10': 0.28,
    'recall@40': 0.42,
    'mrr@10': 0.19
}

print("So sÃ¡nh Quick Test vs Baseline:")
print("=" * 60)
for metric in baseline_metrics:
    baseline = baseline_metrics[metric]
    quick = quick_metrics[metric]
    improvement = (quick - baseline) / baseline * 100
    status = "âœ“" if improvement > 0 else "âœ—"
    print(f"{status} {metric:15s}: {baseline:.4f} â†’ {quick:.4f} ({improvement:+.1f}%)")
EOF
```

---

## ğŸ’¡ TIPS

### 1. Náº¿u muá»‘n nhanh hÆ¡n ná»¯a

```bash
# Chá»‰ 200 queries, 3 epochs (~5 phÃºt)
python train_quick_test.py \
  --num_samples 200 \
  --epochs 3 \
  --device cuda
```

### 2. Náº¿u bá»‹ Out of Memory

```yaml
# Sá»­a config:
training:
  batch_size: 8       # Giáº£m tá»« 16 â†’ 8

rl_agent:
  hidden_dim: 64      # Giáº£m tá»« 128 â†’ 64
```

### 3. Monitor trong khi train

```bash
# Terminal 1: Training
python train_quick_test.py --num_samples 1000 --epochs 10

# Terminal 2: Watch logs
tail -f logs_msa_quick/train.log

# Terminal 3: GPU usage (náº¿u dÃ¹ng GPU)
watch -n 1 nvidia-smi
```

### 4. Táº¯t output Ä‘á»ƒ nhanh hÆ¡n

```bash
# Redirect output
python train_quick_test.py \
  --num_samples 1000 \
  --epochs 10 \
  > /dev/null 2>&1
```

---

## âš™ï¸ TÃ™Y CHá»ˆNH Sá» SAMPLES

### Qua command line (Khuyáº¿n nghá»‹)

```bash
# 500 queries
python train_quick_test.py --num_samples 500 --epochs 5

# 1000 queries
python train_quick_test.py --num_samples 1000 --epochs 10

# 5000 queries
python train_quick_test.py --num_samples 5000 --epochs 10
```

### Hoáº·c sá»­a code `train.py`

ThÃªm vÃ o file `train.py` sau dÃ²ng 343:

```python
# Chá»‰ láº¥y 1000 queries Ä‘áº§u tiÃªn
query_ids = list(train_queries.keys())
np.random.shuffle(query_ids)
query_ids = query_ids[:1000]  # <-- THÃŠM DÃ’NG NÃ€Y
```

---

## ğŸ¯ Lá»†NH HOÃ€N CHá»ˆNH

### Setup (one-time)

```bash
cd /Users/vanhkhongpeo/Documents/Github/Adaptive_information_retrival/adaptive-information-retrieval/adaptive-ir-system

# CÃ i packages
pip install torch numpy scikit-learn h5py rank-bm25 nltk tqdm pyyaml

# NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### Quick Test (10-15 phÃºt)

```bash
# Test nhanh nháº¥t
python train_quick_test.py \
  --config configs/msa_quick_test.yaml \
  --num_samples 500 \
  --epochs 5 \
  --device cuda \
  2>&1 | tee quick_test.log

# Xem káº¿t quáº£
grep "Validation" quick_test.log
grep "best model" quick_test.log
```

### Test Inference

```bash
# Load model vÃ  test
python inference.py \
  --config configs/msa_quick_test.yaml \
  --checkpoint checkpoints_msa_quick/best_model.pt \
  --query "machine learning algorithms"
```

---

## â“ FAQ

### Q: Káº¿t quáº£ cÃ³ Ä‘Ã¡ng tin khÃ´ng?
**A**: Vá»›i 500-1000 queries: Chá»‰ Ä‘á»ƒ verify code. Vá»›i 5000+ queries: ÄÃ¡ng tin hÆ¡n nhÆ°ng váº«n khÃ´ng báº±ng full training.

### Q: CÃ³ nÃªn train full sau khi test?
**A**: NÃªn! Káº¿t quáº£ quick test chá»‰ Ä‘á»ƒ verify. Train full sáº½ tá»‘t hÆ¡n nhiá»u.

### Q: Táº¡i sao reward Ã¢m?
**A**: BÃ¬nh thÆ°á»ng á»Ÿ epoch Ä‘áº§u. RL agent Ä‘ang há»c, chÆ°a tá»‘t hÆ¡n baseline.

### Q: Bao lÃ¢u thÃ¬ tháº¥y improvement?
**A**: Vá»›i 1000 queries: Epoch 5-10. Vá»›i 500 queries: CÃ³ thá»ƒ khÃ´ng tháº¥y.

---

## ğŸ“Š Báº¢NG TÃ“M Táº®T

| Má»¥c Ä‘Ã­ch | Queries | Epochs | Thá»i gian | Lá»‡nh |
|----------|---------|--------|-----------|------|
| **Verify code** | 200-500 | 3-5 | 5-15 min | `--num_samples 500 --epochs 5` |
| **Xem káº¿t quáº£ sÆ¡ bá»™** | 1,000 | 10 | 20-30 min | `--num_samples 1000 --epochs 10` |
| **Káº¿t quáº£ tin cáº­y** | 5,000 | 10 | 1-2 giá» | `--num_samples 5000 --epochs 10` |
| **Production** | 271,345 | 50 | 25-37 giá» | DÃ¹ng `train.py` thÆ°á»ng |

---

**ChÃºc báº¡n test thÃ nh cÃ´ng!** ğŸš€

Sau khi verify code hoáº¡t Ä‘á»™ng, báº¡n cÃ³ thá»ƒ:
1. Train vá»›i nhiá»u queries hÆ¡n (5000-10000)
2. TÄƒng epochs (20-30)
3. Hoáº·c train full vá»›i 271K queries
