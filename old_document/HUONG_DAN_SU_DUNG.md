# ğŸ“š HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG - ADAPTIVE IR TRAINING PIPELINE

## ğŸ¯ Tá»•ng quan

ChÆ°Æ¡ng trÃ¬nh huáº¥n luyá»‡n Adaptive IR System vá»›i cÃ¡c tÃ­nh nÄƒng:
- **3 cháº¿ Ä‘á»™ cháº¡y**: quick, medium, full
- **Checkpoint management**: Tá»± Ä‘á»™ng lÆ°u vÃ  resume
- **Early stopping**: Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
- **Logging**: Log chi tiáº¿t ra file vÃ  console
- **Config linh hoáº¡t**: Qua command line, file JSON, hoáº·c code

---

## ğŸš€ CÃ¡ch cháº¡y nhanh

### 1. Quick Test (~5-10 phÃºt)
```bash
cd /kaggle/adaptive-information-retrieval/adaptive-ir-system
python train_full_epoch.py --mode quick
```

### 2. Medium Training (~30-60 phÃºt)
```bash
python train_full_epoch.py --mode medium
```

### 3. Full Epoch Training (vÃ i giá»)
```bash
python train_full_epoch.py --mode full
```

---

## âš™ï¸ Cáº¥u hÃ¬nh chi tiáº¿t

### Qua Command Line

```bash
# TÃ¹y chá»‰nh epochs vÃ  batch size
python train_full_epoch.py --mode medium --epochs 3 --batch-size 128

# TÃ¹y chá»‰nh learning rate
python train_full_epoch.py --lr 1e-4

# TÃ¹y chá»‰nh evaluation
python train_full_epoch.py --eval-every 500 --num-eval-queries 200

# Táº¯t early stopping
python train_full_epoch.py --no-early-stopping

# Resume tá»« checkpoint
python train_full_epoch.py --resume checkpoints/latest.pt
```

### Qua File JSON

```bash
# Táº¡o config
python -c "
from train_full_epoch import TrainingConfig
config = TrainingConfig(mode='medium')
config.epochs = 3
config.batch_size = 128
config.learning_rate = 1e-4
config.to_json('my_config.json')
"

# Cháº¡y vá»›i config
python train_full_epoch.py --config my_config.json
```

### Chá»‰nh sá»­a trá»±c tiáº¿p trong code

Má»Ÿ file `train_full_epoch.py`, tÃ¬m class `TrainingConfig` vÃ  sá»­a:

```python
@dataclass
class TrainingConfig:
    # =========================================================================
    # CHáº¾ Äá»˜ CHáº Y
    # =========================================================================
    mode: str = 'medium'  # Äá»•i tá»« 'quick' sang 'medium' hoáº·c 'full'
    
    # =========================================================================
    # HUáº¤N LUYá»†N
    # =========================================================================
    epochs: int = 3               # TÄƒng sá»‘ epochs
    batch_size: int = 128         # TÄƒng batch size
    learning_rate: float = 1e-4   # Giáº£m learning rate
```

---

## ğŸ“Š CÃ¡c cháº¿ Ä‘á»™ chi tiáº¿t

| Cháº¿ Ä‘á»™ | Train Queries | Eval Queries | Eval Every | Thá»i gian Æ°á»›c tÃ­nh |
|--------|---------------|--------------|------------|-------------------|
| quick  | 500           | 100          | 200        | 5-10 phÃºt         |
| medium | 5,000         | 300          | 1,000      | 30-60 phÃºt        |
| full   | ALL (~270k)   | 1,000        | 5,000      | 5-10 giá»          |

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c output

```
adaptive-ir-system/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ config.json          # Config Ä‘Ã£ sá»­ dá»¥ng
â”‚   â”œâ”€â”€ latest.pt            # Checkpoint má»›i nháº¥t
â”‚   â”œâ”€â”€ best_model.pt        # Model tá»‘t nháº¥t
â”‚   â””â”€â”€ step_XXXXX.pt        # Checkpoint theo step
â””â”€â”€ logs/
    â”œâ”€â”€ train_YYYYMMDD_HHMMSS.log   # Log file
    â””â”€â”€ training_history.json        # Training history
```

---

## ğŸ”§ CÃ¡c tham sá»‘ quan trá»ng

### 1. Model Architecture

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `embedding_dim` | 500 | KÃ­ch thÆ°á»›c Word2Vec embedding |
| `hidden_dim` | 256 | KÃ­ch thÆ°á»›c hidden layer |
| `num_heads` | 4 | Sá»‘ attention heads |
| `num_layers` | 2 | Sá»‘ Transformer layers |
| `dropout` | 0.1 | Dropout rate |

### 2. Training Hyperparameters

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `batch_size` | 64 | Sá»‘ samples má»—i batch |
| `learning_rate` | 3e-4 | Learning rate |
| `weight_decay` | 0.01 | L2 regularization |
| `max_grad_norm` | 0.5 | Gradient clipping |

### 3. PPO Hyperparameters

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `ppo_epochs` | 3 | PPO update epochs |
| `clip_epsilon` | 0.2 | PPO clipping |
| `gamma` | 0.99 | Discount factor |
| `entropy_coef` | 0.01 | Entropy bonus |
| `update_every` | 512 | Update sau N samples |

### 4. Retrieval

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `max_candidates` | 20 | Candidates cho agent |
| `top_k_retrieve` | 100 | Top-k BM25 results |
| `rrf_k` | 60 | RRF constant |
| `bm25_k1` | 0.9 | BM25 k1 parameter |
| `bm25_b` | 0.4 | BM25 b parameter |

### 5. Early Stopping

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `early_stopping` | True | CÃ³ dÃ¹ng early stopping |
| `patience` | 5 | Sá»‘ láº§n khÃ´ng cáº£i thiá»‡n |
| `min_delta` | 0.001 | NgÆ°á»¡ng cáº£i thiá»‡n tá»‘i thiá»ƒu |

---

## ğŸ“ˆ Theo dÃµi training

### 1. Xem log realtime
```bash
tail -f logs/train_*.log
```

### 2. Xem GPU usage
```bash
watch -n 1 nvidia-smi
```

### 3. Load training history
```python
import json
with open('logs/training_history.json') as f:
    history = json.load(f)

# Plot rewards
import matplotlib.pyplot as plt
steps = [h['step'] for h in history['train_history']]
rewards = [h['reward'] for h in history['train_history']]
plt.plot(steps, rewards)
plt.xlabel('Step')
plt.ylabel('Reward')
plt.show()
```

---

## ğŸ”„ Resume Training

```bash
# Resume tá»« checkpoint má»›i nháº¥t
python train_full_epoch.py --resume checkpoints/latest.pt

# Resume tá»« best model
python train_full_epoch.py --resume checkpoints/best_model.pt

# Resume vá»›i config khÃ¡c
python train_full_epoch.py --resume checkpoints/latest.pt --epochs 5
```

---

## ğŸ§ª Evaluation riÃªng

```python
from train_full_epoch import *

# Load model
config = TrainingConfig.from_json('checkpoints/config.json')
data = DataManager(config)
data.load_all()

search = BM25SearchEngine(data, config)
search.build_index()

agent = QueryReformulationAgent(config).to('cuda')
checkpoint = torch.load('checkpoints/best_model.pt')
agent.load_state_dict(checkpoint['model_state_dict'])

# Evaluate
trainer = Trainer(agent, data, search, config)
metrics = trainer.evaluate('test', max_queries=1000)
print(metrics)
```

---

## â— Xá»­ lÃ½ lá»—i thÆ°á»ng gáº·p

### 1. Out of Memory (OOM)
```bash
# Giáº£m batch size
python train_full_epoch.py --batch-size 32
```

### 2. Training quÃ¡ cháº­m
```bash
# TÄƒng update_every Ä‘á»ƒ giáº£m PPO updates
# Sá»­a trong code: update_every = 1024
```

### 3. Reward khÃ´ng tÄƒng
- Thá»­ giáº£m learning rate: `--lr 1e-4`
- TÄƒng entropy coefficient trong config

### 4. Early stopping quÃ¡ sá»›m
```bash
# Táº¯t early stopping
python train_full_epoch.py --no-early-stopping

# Hoáº·c tÄƒng patience trong config
```

---

## ğŸ“ VÃ­ dá»¥ configs

### Config cho GPU yáº¿u (4GB)
```python
config = TrainingConfig(
    mode='quick',
    batch_size=16,
    max_candidates=10,
    update_every=256
)
```

### Config cho training dÃ i
```python
config = TrainingConfig(
    mode='full',
    epochs=5,
    batch_size=128,
    learning_rate=1e-4,
    early_stopping=False
)
```

### Config cho debugging
```python
config = TrainingConfig(
    mode='quick',
    eval_every=50,
    log_every=10,
    save_every=100
)
```

---

## ğŸ¯ Tips & Best Practices

1. **Báº¯t Ä‘áº§u vá»›i mode='quick'** Ä‘á»ƒ verify setup hoáº¡t Ä‘á»™ng
2. **Monitor GPU memory** vá»›i `nvidia-smi`
3. **Check baseline metrics** trÆ°á»›c khi train dÃ i
4. **Save config** Ä‘á»ƒ reproduce experiments
5. **Use early stopping** Ä‘á»ƒ trÃ¡nh overfitting
6. **Log everything** Ä‘á»ƒ debug

---

## ğŸ“ Troubleshooting

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check logs trong `logs/train_*.log`
2. Verify data paths trong config
3. Check GPU memory vá»›i `nvidia-smi`
4. Try vá»›i mode='quick' trÆ°á»›c

---

*ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸš€*
