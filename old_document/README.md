# Adaptive Retrieve-Fuse-Re-rank Search Engine

Deep Reinforcement Learning Query Reformulation with RRF Fusion and BERT Re-ranking

## ğŸ¯ Overview

This system implements an advanced Information Retrieval pipeline combining:
- **RL Query Reformulation**: Learned policy for query expansion
- **Multi-Query Retrieval**: Parallel BM25 retrievals
- **RRF Fusion**: Reciprocal Rank Fusion for result merging
- **BERT Re-ranking**: Cross-encoder for final ranking

## ğŸ“ Project Structure

```
adaptive-ir-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ candidate_mining/    # Stage 0: Term extraction
â”‚   â”œâ”€â”€ rl_agent/            # Stage 1: RL reformulation
â”‚   â”œâ”€â”€ fusion/              # Stage 2: RRF fusion
â”‚   â”œâ”€â”€ reranker/            # Stage 3: BERT re-ranking
â”‚   â”œâ”€â”€ evaluation/          # Metrics computation
â”‚   â””â”€â”€ utils/               # Helper functions
â”œâ”€â”€ configs/                 # Configuration files
â”œâ”€â”€ data/                    # Datasets
â”œâ”€â”€ models/                  # Trained models
â”œâ”€â”€ scripts/                 # Training/evaluation scripts
â””â”€â”€ tests/                   # Unit tests
```

## ğŸš€ Quick Start

### Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download MS MARCO dataset
python scripts/download_msmarco.py

# Build search index
python scripts/build_index.py
```

### Training

```bash
# Train with default configuration
python train.py --config configs/default_config.yaml

# Train on GPU with custom settings
python train.py \
    --config configs/default_config.yaml \
    --device cuda \
    --epochs 50 \
    --test
```

### Inference

```bash
# Interactive mode
python inference.py \
    --config configs/default_config.yaml \
    --checkpoint checkpoints/best_model.pt \
    --interactive

# Single query
python inference.py \
    --config configs/default_config.yaml \
    --checkpoint checkpoints/best_model.pt \
    --query "what is machine learning"
```

## ğŸ“š Detailed Usage

### Using Legacy Datasets (Jeopardy, TREC-CAR, MS Academic)

This system is compatible with the original dl4ir-query-reformulator datasets:

**1. Test Legacy Data Loading:**
```bash
# Verify all HDF5 files load correctly
python scripts/test_legacy_data.py
```

**2. Train on Legacy Dataset:**
```bash
# Train on TREC-CAR dataset
python train.py \
    --config configs/legacy_config.yaml \
    --device cuda

# Train on Jeopardy dataset
python train.py --config configs/legacy_config.yaml --device cuda
# (Edit legacy_config.yaml to set dataset_type: 'jeopardy')
```

**Legacy Dataset Structure:**
- **TREC-CAR**: `trec_car_dataset.hdf5` (queries = article sections)
- **Jeopardy**: `jeopardy_dataset.hdf5`, `jeopardy_corpus.hdf5` (queries = questions)
- **MS Academic**: `msa_dataset.hdf5`, `msa_corpus.hdf5` (queries = paper titles)
- **Embeddings**: `D_cbow_pdw_8B.pkl` (374K Word2Vec embeddings, 500-dim)

### Using MS MARCO Dataset

### 1. Data Preparation

**Download MS MARCO:**
```bash
# Download all data (collection + queries + qrels)
python scripts/download_msmarco.py --data_dir ./data/msmarco

# Verify download
python scripts/download_msmarco.py --data_dir ./data/msmarco --verify

# Print statistics
python scripts/download_msmarco.py --data_dir ./data/msmarco --stats
```

**Build Pyserini Index:**
```bash
# Build BM25 index (~30 minutes)
python scripts/build_index.py \
    --collection ./data/msmarco/collection.tsv \
    --index ./data/msmarco/index \
    --threads 8
```

### 2. Training

**Basic Training:**
```bash
python train.py --config configs/default_config.yaml
```

**Advanced Options:**
```bash
python train.py \
    --config configs/default_config.yaml \
    --device cuda \              # Use GPU
    --epochs 50 \                # Number of epochs
    --seed 42 \                  # Random seed
    --test                       # Run test evaluation after training
```

**Resume from Checkpoint:**
```bash
python train.py \
    --config configs/default_config.yaml \
    --checkpoint checkpoints/checkpoint_epoch_10.pt
```

**Training Outputs:**
- Checkpoints: `checkpoints/checkpoint_epoch_*.pt`
- Best model: `checkpoints/best_model.pt`
- Training logs: `logs/train.log`
- Test results: `checkpoints/test_results.json`

### 3. Inference

**Interactive Mode:**
```bash
python inference.py \
    --config configs/default_config.yaml \
    --checkpoint checkpoints/best_model.pt \
    --interactive
```

**Single Query:**
```bash
python inference.py \
    --config configs/default_config.yaml \
    --checkpoint checkpoints/best_model.pt \
    --query "what is machine learning" \
    --top_k 10
```

**Batch Queries:**
```bash
python inference.py \
    --config configs/default_config.yaml \
    --checkpoint checkpoints/best_model.pt \
    --queries_file data/msmarco/queries.dev.tsv \
    --output results/dev_results.json \
    --top_k 100
```

```bash
# Train RL agent
python scripts/train_rl_agent.py --config configs/train_config.yaml

# Full pipeline evaluation
python scripts/evaluate_pipeline.py --config configs/eval_config.yaml
```

## ğŸ“Š Performance

| Method | Recall@100 | MRR@10 | nDCG@10 | Latency (ms) |
|--------|------------|--------|---------|--------------|
| BM25 | 65% | 18% | 22% | 20 |
| BM25+RM3 | 72% | 19% | 23% | 35 |
| **Ours** | **78%** | **25%** | **30%** | 140 |

## ğŸ”¬ Research Questions

1. Does RL reformulation outperform RM3?
2. Does RRF improve OOD robustness?
3. What's the Recall/Latency tradeoff for different m?
4. What term selection patterns does the agent learn?

## ğŸ“š Citation

Based on:
- Nogueira & Cho (2017): Task-Oriented Query Reformulation with RL
- Cormack et al. (2009): Reciprocal Rank Fusion
- Nogueira & Cho (2019): Passage Re-ranking with BERT

## ğŸ“„ License

BSD 3-Clause License (inherited from original QueryReformulator)
